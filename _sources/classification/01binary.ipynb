{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 線形二値分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二値分類の例：スパム判定\n",
    "\n",
    "$\\def\\bm{\\boldsymbol}$電子メールは誰にでもすぐにメッセージを送ることができるので便利である一方、自分が受け取りたくないスパムメール（迷惑メール）が送られてくることがある。[総務省の統計](https://www.soumu.go.jp/main_sosiki/joho_tsusin/d_syohi/m_mail.html)によると、電気通信事業者１０社の全受信メール数に対する[迷惑メール数の割合は約50%（2020年3月時点）](https://www.soumu.go.jp/main_content/000693529.pdf)である。そこで、多くのメールサーバやメールクライアントではスパムメールを自動で認識し、利用者の目に触れさせない機能（スパムフィルタ）が搭載されている。スパムフィルタの主なタスクは、与えられたメールがスパムであるか、スパムではないか自動的に判定することである。この判定を行うモジュール、すなわちスパム判定器を構築するのが、今回のお題である。\n",
    "\n",
    "では、スパム判定器をどのように構築すればよいか。以下のスパムメールを具体例として考えたい。\n",
    "\n",
    "![spam](fig/spam-s.png \"スパムメールの例\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このメールの本文中で\"I am Victoriya\", \"search for boy-friend\", \"Ny photos\"などのフレーズが出てくることに着目し、これらのフレーズが本文中に含まれているメールをスパムと判定するには、次のようなルールを実装するかもしれない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_spam(x):\n",
    "    if x.find('I am Victoriya') != -1:\n",
    "        return True\n",
    "    if x.find('search for boy-friend') != -1:\n",
    "        return True\n",
    "    if x.find('Ny photos') != -1:\n",
    "        return True\n",
    "    #\n",
    "    # ... (大量の判定ルール)\n",
    "    #\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_spam('I am Victoriya, I am 27 y.o.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_spam('I search for boy-friend.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このようにメールの本文に関して、スパムである条件を記述していくアプローチを**ルール**に基づく手法と呼ぶ。ルールに基づく手法は、訓練データが少ないときは迅速に作ることができる、スパムと判定する条件が明解であるという利点がある。一方で、スパムと判定する条件を詳細化するほどプログラムが複雑になるため、ルールの保守が難しい。また、英語のスパムメールのために作ったルールを、日本語のスパムメールに適用することができないため、特定の言語やジャンルにおけるメールにしか対応できない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そこで、教師あり学習を用いてスパム判定器を構築することを考える。より具体的には、メールを$d$次元の説明変数$\\bm{x} \\in \\mathbb{R}^d$で表現し、スパムメールか（$1$）スパムメールでないか（$0$）を表す目的変数（出力）$y \\in \\{0, 1\\}$を考える。説明変数から目的変数を計算する関数$f: \\mathbb{R}^d \\longmapsto \\{0, 1\\}$を教師データ$\\mathcal{D}$から学習することで、スパム判定器を構築できる。説明変数から$\\{0, 1\\}$などの離散値を取る目的変数を求めることを**分類**（classification）もしくは**識別**と呼ぶ。分類において目的変数が取りうる値は、**クラス**や**カテゴリ**などと呼ばれる。特に、説明変数の取りうる値が２つのクラスに限定された分類を、**二値分類**（binary classification）と呼ぶ。\n",
    "\n",
    "以下の図は、機械学習によるスパムフィルターの典型的な動作を示している。スパムメールに関する教師データを使い、事前にスパム判定器を学習しておく。そして、与えられたメールに対して、スパム判定器はメールがスパムであるかどうか推定する。スパムと判定されたメールはスパムフォルダーに自動的に仕分けすることで、新着メールとして表示させないようにする。ただ、スパム判定器が間違った判定をしてしまうことがある。例えば、本当はスパムであるメールをスパムではないと判定してしまうと、新着メールとして表示されてしまう。このとき、メールを閲覧したユーザがそのメールにスパムであることの目印（フラグ）を付けたとする（スパムフォルダーに移動させてもよい）。これは、スパムメールの新しい学習事例を作ったことに相当する。そこで、この新しい学習事例を教師データとしてスパム判定器を再学習すると、スパム判定の性能が向上すると期待される。以降では、スパム判定などの二値分類を実現するモデルと、その学習方法や評価方法を説明する。\n",
    "\n",
    "<img src=\"fig/spam-filter.svg\" alt=\"スパム判定器\" width=\"480px\">\n",
    "\n",
    "なお、スパムの語源はイギリスBBCが1970年頃に制作した『[空飛ぶモンティ・パイソン](https://ja.wikipedia.org/wiki/%E7%A9%BA%E9%A3%9B%E3%81%B6%E3%83%A2%E3%83%B3%E3%83%86%E3%82%A3%E3%83%BB%E3%83%91%E3%82%A4%E3%82%BD%E3%83%B3)』のスケッチ「[スパム](https://ja.wikipedia.org/wiki/%E3%82%B9%E3%83%91%E3%83%A0_(%E3%83%A1%E3%83%BC%E3%83%AB))」から来ていると言われている（以下は2014年に行われた[復活ライブ](https://ja.wikipedia.org/wiki/%E3%83%A2%E3%83%B3%E3%83%86%E3%82%A3%E3%83%BB%E3%83%91%E3%82%A4%E3%82%BD%E3%83%B3_%E5%BE%A9%E6%B4%BB%E3%83%A9%E3%82%A4%E3%83%96!)の時に使われた\n",
    "メニュー）。\n",
    "\n",
    "<a title=\"Eduardo Unda-Sanzana from Antofagasta, Chile, CC BY 2.0 &lt;https://creativecommons.org/licenses/by/2.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Monty_Python_Live_02-07-14_13_04_42_(14598710791).jpg\"><img width=\"256\" alt=\"Monty Python Live 02-07-14 13 04 42 (14598710791)\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/8e/Monty_Python_Live_02-07-14_13_04_42_%2814598710791%29.jpg/256px-Monty_Python_Live_02-07-14_13_04_42_%2814598710791%29.jpg\"></a>\n",
    "\n",
    "メールのスパム判定以外にも、二値分類には様々な応用例がある。\n",
    "\n",
    "+ 臨床検査: 血液検査やアンケートの回答などの説明変数から、患者の病気や異常の有無を判定する\n",
    "+ 与信調査: 属性情報や過去の取引履歴から顧客の信用の有無を判定する\n",
    "+ 当落予測: 世論調査や出口調査の結果から、候補者の当選・落選を予測する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 線形二値分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**線形二値分類** (linear binary classification) は、$d$次元の特徴ベクトルで表現された事例$\\bm{x} \\in \\mathbb{R}^{d}$が与えられた時、線形モデルのパラメータ$\\bm{w} \\in \\mathbb{R}^{d}$との内積を計算し、その正負によってラベル$\\hat{y} \\in \\{1, 0\\}$を推定する。\n",
    "\n",
    ":::{admonition} 線形二値分類のラベル推定式\n",
    ":class: important\n",
    "$$\n",
    "\\begin{gather}\n",
    "    \\hat{y} = \\begin{cases}\n",
    "        1 & \\left(\\bm{x}^\\top \\bm{w} > 0\\right) \\\\\n",
    "        0 & (\\mbox{それ以外})\n",
    "    \\end{cases}\n",
    "\\end{gather}\n",
    "$$ (eq:binary-classification)\n",
    ":::\n",
    "\n",
    "スパム判定では、スパムメールを$\\hat{y} = 1$、スパムではないメールを$\\hat{y} = 0$などと定義する。二値分類において、$\\hat{y} = 1$の事例を**正例**（positive example）、$\\hat{y} = 0$の事例を**負例**（negative example）と呼ぶことがある。\n",
    "\n",
    "メールがスパムであるか判定するための手がかりは色々考えられるが、ここでは簡単のため、$d=9$次元の特徴空間を例として用いる。この特徴空間の$1$次元目は、与えられたメールの本文に\"attached\"という単語が含まれるならば$1$、含まれなければ$0$とする。同様に、$2$次元目から$8$次元目まで、メール本文中に、それぞれ\"darling\", \"file\", \"hi\", \"kyoto\", \"mark\", \"my\", \"photo\"という単語が含まれるかどうかを表現する。$9$次元目はメールの内容に関わらず常に$1$とする。例えば、スパム判定を行いたいメールの本文が\"Hi darling, my photo in attached file\"であった場合、事例$\\bm{x} \\in \\mathbb{R}^9$は、\n",
    "\n",
    "\\begin{gather}\n",
    " \\bm{x} = \\begin{pmatrix}\n",
    "1 & 1 & 1 & 1 & 0 & 0 & 1 & 1 & 1\n",
    "\\end{pmatrix}^\\top\n",
    "\\end{gather}\n",
    "\n",
    "線形二値分類は、事例$\\bm{x}$とパラメータ$\\bm{w}$の内積、\n",
    "\\begin{gather}\n",
    " \\bm{x}^\\top \\bm{w} = \\begin{pmatrix}\n",
    "1 & 1 & 1 & 1 & 0 & 0 & 1 & 1 & 1\n",
    "\\end{pmatrix} \\begin{pmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "w_3 \\\\\n",
    "w_4 \\\\\n",
    "w_5 \\\\\n",
    "w_6 \\\\\n",
    "w_7 \\\\\n",
    "w_8 \\\\\n",
    "w_9\n",
    "\\end{pmatrix} =w_1 + w_2 + w_3 + w_4 + w_7 + w_8 + w_9\n",
    "\\end{gather}\n",
    "を計算し、その符号が正ならば事例をスパム（$\\hat{y} = 1$）と判定し、$0$以下ならばスパムでない（$\\hat{y} = 0$）と判定する。つまり、メールに含まれている単語$j$に対応する重み$w_j$の和でメールのスパムらしさをスコア付けし、そのスコアがしきい値$0$を超えたらスパムメールと判定する。\n",
    "\n",
    "なお、特徴ベクトルの$d$次元目の値が常に$1$であることに注意して、正例・負例の判別条件を求めると、\n",
    "\\begin{align}\n",
    " \\bm{x}^\\top\\bm{w} = \\sum_{j=1}^{d} w_j x_j = \\sum_{j=1}^{d-1} w_j x_j + w_d &> 0 \\\\\n",
    " \\sum_{j=1}^{d-1} w_j x_j &> -w_d\n",
    "\\end{align}\n",
    "となる。すなわち、事例を分類するしきい値を$0$に固定するのではなく、重み$-w_9$でしきい値を自動的に調整する効果が得られる。このように、すべての事例で$1$となる特徴量を入れておくことで、線形二値分類モデルのバイアス項を導入できる。\n",
    "\n",
    "線形モデルのパラメータ$\\bm{w}$は、学習データによく合致するように（例えば学習データ上においてスパム判定が正しく行えるように）決定する。モデルのパラメータ$\\bm{w}$を推定する方法は色々あるが、ここではロジスティック回帰に基づく確率的勾配降下法を紹介する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ロジスティック回帰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ロジスティック回帰**（logistic regression）は線形二値分類を実現するモデルの一つで、事例$\\bm{x}$に対するラベル$\\hat{y} \\in \\{1, 0\\}$の条件付き確率$p(y|\\bm{x})$を以下の式で求める。\n",
    "\n",
    ":::{admonition} ロジスティック回帰\n",
    ":class: important\n",
    "\\begin{align}\n",
    " P(\\hat{y} = 1|\\bm{x}) &= \\sigma(\\bm{x}^\\top \\bm{w}) = \\frac{1}{1 + \\exp\\left(-\\bm{x}^\\top \\bm{w}\\right)} \\\\\n",
    " P(\\hat{y} = 0|\\bm{x}) &= 1 - P(\\hat{y} = 1|\\bm{x})\n",
    "\\end{align}\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ただし、$\\sigma(a)$は**シグモイド関数** (sigmoid function) である。\n",
    "\n",
    ":::{admonition} シグモイド関数\n",
    ":class: important\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\sigma(a) = \\frac{1}{1 + \\exp\\left(-a\\right)}\n",
    "\\end{gather}\n",
    "$$ (eq:sigmoid)\n",
    ":::\n",
    "\n",
    "シグモイド関数の形状を以下に示す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "ax.plot(x, 1 / (1 + np.exp(-x)))\n",
    "ax.set_xlabel('$a$')\n",
    "ax.set_ylabel('$\\sigma(a)$')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この形状から明らかなように、シグモイド関数は、以下の特徴を持つ。\n",
    "\n",
    "+ $(-\\infty, +\\infty) \\to (0, 1)$の単調増加関数\n",
    "+ $\\lim_{a \\to -\\infty} \\sigma(a) = 0$\n",
    "+ $\\lim_{a \\to +\\infty} \\sigma(a) = 1$\n",
    "+ $(0, 0.5)$に関して点対称"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ゆえに、ロジスティック回帰は線形二値分類モデルの内積値$\\bm{x}^\\top\\bm{w}$をシグモイド関数$\\sigma$で確率値に変換していると理解することができる。また、\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    " 1 - \\sigma(a) = \\frac{\\left\\{1 + e^{-a}\\right\\} - 1}{1 + e^{-a}} = \\frac{e^{-a}}{1 + e^{-a}} = \\frac{e^a e^{-a}}{e^a(1 + e^{-a})} = \\frac{1}{1 + e^a} = \\sigma(-a)\n",
    "\\end{align}\n",
    "$$ (eq:sigmoid-negative)\n",
    "\n",
    "であるから、\n",
    "\n",
    "\\begin{align}\n",
    " P(\\hat{y} = 1|\\bm{x}) &= \\sigma(\\bm{x}^\\top \\bm{w}) \\\\\n",
    " P(\\hat{y} = 0|\\bm{x}) &= 1 - P(\\hat{y} = 1|\\bm{x}) = 1 - \\sigma(\\bm{x}^\\top \\bm{w}) = \\sigma(-\\bm{x}^\\top \\bm{w})\n",
    "\\end{align}\n",
    "\n",
    "なお、事例$\\bm{x}$を$\\hat{y}=1$と予測する確率が$0.5$を上回る条件を求めると、線形二値分類の判別式{eq}`eq:binary-classification`と整合することが確認できる。\n",
    "\\begin{align}\n",
    " P(\\hat{y} = 1|\\bm{x}) > 0.5 \\Leftrightarrow \\frac{1}{1 + \\exp\\left(-\\bm{x}^\\top \\bm{w}\\right)} > \\frac{1}{2} \\Leftrightarrow 2 > 1 + \\exp\\left(-\\bm{x}^\\top \\bm{w}\\right) \\Leftrightarrow \\bm{x}^\\top \\bm{w} > 0\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### シグモイド関数の実装\n",
    "\n",
    "シグモイド関数$\\sigma(a)$を素直に実装すると、以下のようなプログラムになるであろう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "    return 1 / (1 + np.exp(-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9525741268224334, 0.04742587317756678)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(3), sigmoid(-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ところが、$a=-1000$とすると$e^{1000}$の計算でオーバーフローが発生する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-d5641a12eae7>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-a))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(-1000.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この問題を回避するには、式{eq}`eq:sigmoid-negative`より$\\sigma(-a) = 1 - \\sigma(a)$であることを利用し、`np.exp`の計算結果が大きくならないようにすればよい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "    if 0 <= a:\n",
    "        return 1 / (1 + np.exp(-a))\n",
    "    else:\n",
    "        return 1. - 1 / (1 + np.exp(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(-1000.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの表現\n",
    "\n",
    "分類問題のデータの表現方法は回帰と同様である。説明変数と目的変数の一つの組を学習事例として表現する。$1$番目の学習事例を$(\\bm{x}_1, y_1)$、$2$番目の学習事例を$(\\bm{x}_2, y_2)$、$i$番目の学習事例を$(\\bm{x}_i, y_i)$と表すことにすると、$N$個の事例からなるデータ$\\mathcal{D}$は次のように表される。\n",
    "\\begin{align}\n",
    "\\mathcal{D} = \\left\\{(\\bm{x}_1, y_1), (\\bm{x}_2, y_2), \\dots, (\\bm{x}_N, y_N)\\right\\} = \\left\\{(\\bm{x}_i, y_i)\\right\\}_{i=1}^{N}\n",
    "\\end{align}\n",
    "\n",
    "以降では、2個の学習事例（$N=2$）からなる単純なデータ$\\mathcal{D}_{s}$を例として用いる。\n",
    "\\begin{align}\n",
    "\\mathcal{D}_s &= \\left\\{(\\bm{x}_i, y_i)\\right\\}_{i=1}^{2} = \\left\\{(\\bm{x}_1, y_1), (\\bm{x}_2, y_2)\\right\\} \\\\\n",
    " (\\bm{x}_1, y_1) &= \\left(\\begin{pmatrix}\n",
    "1 & 0 & 1 & 1 & 1 & 1 & 0 & 1 & 1\n",
    "\\end{pmatrix}^\\top, 0\\right) \\\\\n",
    " (\\bm{x}_2, y_2) &= \\left(\\begin{pmatrix}\n",
    "1 & 1 & 1 & 1 & 0 & 0 & 1 & 1 & 1\n",
    "\\end{pmatrix}^\\top, 1\\right)\n",
    "\\end{align}\n",
    "\n",
    "なお、学習データ$\\mathcal{D}_{s}$はトイ・データではあるが、スパム判定の例と対応付けて理解できるように、メールを$\\mathcal{D}_{s}$に変換するまでの過程を簡単に説明しておく。先ほどの$d=9$次元の特徴空間を用いたスパムメール判定の例の通り、メールは$9$次元の事例ベクトル$\\bm{x}$で表現され、$1$次元目から$8$次元目まで、各次元はメール本文中に\"attached\", \"darling\", \"file\", \"hi\", \"kyoto\", \"mark\", \"my\", \"photo\"が含まれるならば$1$、含まれなければ$0$である。$9$次元目はメールの内容に関わらず常に$1$としておく。いま、スパム判定器の学習データとして、以下の2つのメールがあるとすると、先ほどの学習データ$\\mathcal{D}_s$が得られる。\n",
    "\n",
    "+ 学習事例1（スパムではない）: \"Hi Mark, Kyoto photo in attached file\"\n",
    "+ 学習事例2（スパム）: \"Hi darling, my photo in attached file\"\n",
    "\n",
    "なお、分類モデルは特徴ベクトル（説明変数）を通してのみ、メールなどの入力を観測できる。したがって、分類が成功しやすくなるような特徴空間を定義することは、分類器の性能を向上させるために極めて重要である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 尤度\n",
    "\n",
    "さて、何らかの方法でモデルのパラメータ$\\bm{w}$を以下のように決定したとしよう。\n",
    "\n",
    "\\begin{align}\n",
    "\\bm{w} = \\begin{pmatrix}\n",
    "1 & 2 & 0 & 0 & 0 & -1 & 1 & 1 & -2\n",
    "\\end{pmatrix}^\\top\n",
    "\\end{align}\n",
    "\n",
    "ここで、$\\mathcal{D}_s$中の事例$\\bm{x}_1$に対してロジスティック回帰モデルを適用する。\n",
    "\n",
    "\\begin{gather}\n",
    " \\bm{x}_1^\\top \\bm{w} = \\begin{pmatrix}\n",
    "1 & 0 & 1 & 1 & 1 & 1 & 0 & 1 & 1\n",
    "\\end{pmatrix} \\begin{pmatrix}\n",
    "1 & 2 & 0 & 0 & 0 & -1 & 1 & 1 & -2\n",
    "\\end{pmatrix}^\\top = -1 \\\\\n",
    "P(\\hat{y} = 1|\\bm{x}_1) = \\sigma(\\bm{x}_1^\\top \\bm{w}) = \\sigma(-1) \\approx 0.269\n",
    "\\end{gather}\n",
    "\n",
    "ゆえに、$P(\\hat{y} = 1|\\bm{x}_1) = 0.269$であるから、このモデルは与えられたメールがスパムである確率を$0.269$と予測し、その確率が$0.5$を超えないことから、$\\bm{x}_1$はスパムメールではないと判定している。\n",
    "\n",
    "これまでの流れは、事例$\\bm{x}_1$とモデルのパラメータ$\\bm{w}$が与えられた時、予測結果$P(\\hat{y} = 1|\\bm{x}_1) = 0.269$を計算するものであった。ここで、学習事例$(\\bm{x}_1, y_1)$は不変（\"Hi Mark, Kyoto photo in attached file\"というメールがスパムではないのは事実）であると考える。そして、確率に対する見方を変えて、このモデルのパラメータ$\\bm{w}$が与えられた学習事例を正しく判定できる確率、すなわちモデルパラメータ$\\bm{w}$の学習事例$(\\bm{x}, y)$に対する**尤度**$\\hat{l}_{\\bm{x}, y}(\\bm{w})$を次式で定義する。\n",
    "\n",
    "\\begin{gather}\n",
    "\\hat{l}_{\\bm{x}, y}(\\bm{w}) = P(\\hat{y} = y | \\bm{x})\n",
    "\\end{gather}\n",
    "\n",
    "この学習事例に対する尤度は、モデルのパラメータが「どのくらい学習事例を正しく再現できるか」を定量化した指標と見なすことができる。尤度が$1$に近いほど学習事例を正しく再現できていること、$0$に近づくほど学習事例を間違って（例えば$y=0$なのに$\\hat{y}=1$と予測して）再現していることを意味する。\n",
    "\n",
    "例えば、学習事例$(\\bm{x}_1, y_1)$は$y_1 = 0$であるから、この学習事例に対するモデルパラメータ$\\bm{w}$の尤度は、\n",
    "\n",
    "\\begin{gather}\n",
    "\\hat{l}_{\\bm{x}_1, y_1}(\\bm{w}) = P(\\hat{y} = 0|\\bm{x}_1) = 1 - P(\\hat{y} = 1|\\bm{x}_1) = 1 - 0.269 = 0.731\n",
    "\\end{gather}\n",
    "\n",
    "これは、モデルパラメータ$\\bm{w}$が学習事例$(\\bm{x}_1, y_1)$を$0.731$の確率で正しく分類できることを表している。\n",
    "\n",
    "続いて、$\\mathcal{D}_s$中の事例$\\bm{x}_2$に対してロジスティック回帰モデルを適用する。\n",
    "\n",
    "\\begin{gather}\n",
    " \\bm{x}_2^\\top \\bm{w} = \\begin{pmatrix}\n",
    "1 & 1 & 1 & 1 & 0 & 0 & 1 & 1 & 1\n",
    "\\end{pmatrix} \\begin{pmatrix}\n",
    "1 & 2 & 0 & 0 & 0 & -1 & 1 & 1 & -2\n",
    "\\end{pmatrix}^\\top = 3 \\\\\n",
    "P(\\hat{y} = 1|\\bm{x}_2) = \\sigma(\\bm{x}_2^\\top \\bm{w}) = \\sigma(3) \\approx 0.953\n",
    "\\end{gather}\n",
    "\n",
    "ゆえに、このモデルは与えられたメールがスパムである確率を$0.953$と推定し、その確率が$0.5$を超えていることから、$\\bm{x}_2$はスパムメールである可能性が高いと判定している。また、モデルパラメータ$\\bm{w}$の学習事例$(\\bm{x}_2, y_2)$に対する尤度$\\hat{l}_{\\bm{x}_2, y_2}(\\bm{w})$は、\n",
    "\\begin{gather}\n",
    "\\hat{l}_{\\bm{x}_2, y_2}(\\bm{w}) = P(\\hat{y} = 1|\\bm{x}_2) = 0.953\n",
    "\\end{gather}\n",
    "\n",
    "これは、モデルパラメータ$\\bm{w}$が学習事例$(\\bm{x}_2, y_2)$を$0.953$の確率で正しく分類できることを表している。\n",
    "\n",
    "なお、学習事例に対する尤度$\\hat{l}_{\\bm{x}, y}$は、$y=0$と$y=1$で場合分けする代わりに、$y$乗を使ってまとめると、次式で表現できる。\n",
    "\n",
    ":::{admonition} 事例ごとの尤度\n",
    ":class: important\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\hat{l}_{\\bm{x}, y}(\\bm{w}) = P(\\hat{y} = y | \\bm{x}) = \\begin{cases}\n",
    "P(\\hat{y} = 1 | \\bm{x}) & (y = 1\\mbox{のとき}) \\\\\n",
    "P(\\hat{y} = 0 | \\bm{x}) & (y = 0\\mbox{のとき}) \\\\\n",
    "\\end{cases}\n",
    "&= p^{y} (1-p)^{(1 - y)}\n",
    "\\end{gather}\n",
    "$$ (eq:instance-likelihood)\n",
    ":::\n",
    "\n",
    "ここで、\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p = P(\\hat{y} = 1 | \\bm{x}) = \\sigma(\\bm{x}^\\top \\bm{w})\n",
    "\\end{align}\n",
    "$$ (eq:definition-of-p)\n",
    "\n",
    "とおいた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最尤推定\n",
    "\n",
    "先ほどの例では、モデルのパラメータ$\\bm{w}$を合理的な値に（手で）調整しておいたので、すべての学習事例を正しく分類できた。しかし、パラメータ$\\bm{w}$の値によっては、正しく分類できない学習事例が出てくる。また、両方の学習事例の尤度は学習事例を完全に再現できる値（$1$）を下回っている。パラメータ$\\bm{w}$を調整することで、すべての学習事例の尤度を$1$に引き上げ、未知の事例に対する予測性能を向上させることができるかもしれない。先ほどの学習データ$\\mathcal{D}_s$の例では、$\\bm{w}$をうまく調整することで、\n",
    "\\begin{gather}\n",
    "\\hat{l}_{\\bm{x}_1, y_1}(\\bm{w}) &= 0.731 \\rightsquigarrow 1 \\\\\n",
    "\\hat{l}_{\\bm{x}_2, y_2}(\\bm{w}) &= 0.953 \\rightsquigarrow 1\n",
    "\\end{gather}\n",
    "を実現できるかもしれない。\n",
    "\n",
    "そこで、学習データ$\\mathcal{D}$全体における尤度を定義し、モデルのパラメータ$\\bm{w}$がどのくらい学習データ$\\mathcal{D}$をうまく反映しているのか、定量的に示したい。ここで、学習データのすべての事例は**独立同分布**（i.i.d: independent and identically distributed）である仮定し、学習データ全体の尤度$\\hat{L}_{\\mathcal{D}}(\\bm{w})$を各学習事例の尤度の結合確率として定義する。\n",
    "\\begin{align}\n",
    "\\hat{L}_{\\mathcal{D}}(\\bm{w}) = \\prod_{i=1}^N \\hat{l}_{\\bm{x}_i, y_i}(\\bm{w})\n",
    "\\end{align}\n",
    "学習データ全体の尤度も$0$から$1$までの値をとり、尤度が$1$に近いほど学習事例を正しく再現できていることを表す。ゆえに、$\\hat{L}_{\\mathcal{D}}(\\bm{w})$を目的関数とみなし、この目的関数の値を最大化するような$\\bm{w}^*$を求めることで、学習データ$\\mathcal{D}$によく合致するモデルパラメータを求めることができる。尤度が最大になるパラメータを求めることを**最尤推定**（MLE: Maximum Likelihood Estimation）と呼ぶ。\n",
    "\n",
    "ところで、学習データ全体の尤度は事例の尤度の積であるから、学習事例の数が多くなると$[0, 1]$の範囲の積を繰り返すことになる。これは、コンピュータ上で学習データ上の尤度を計算するとき、アンダーフローの問題（小さい値を精度良く表現できない問題）を引き起こす。そこで、尤度を最大化する代わりに、尤度の対数をとった**対数尤度**を最大化する。対数尤度は、学習事例の尤度の対数$\\log \\hat{l}_{(\\bm{x}_i, y_i)}(\\bm{w})$の和として表現できる。\n",
    "\\begin{align}\n",
    "\\log \\hat{L}_{\\mathcal{D}}(\\bm{w}) &= \\log \\prod_{i=1}^N \\hat{l}_{\\bm{x}_i, y_i}(\\bm{w})\n",
    "= \\sum_{i=1}^N \\log \\hat{l}_{\\bm{x}_i, y_i}(\\bm{w})\n",
    "\\end{align}\n",
    "\n",
    "なお、回帰では目的関数を最小にするパラメータを求めた。二値分類でも目的関数の最小化の問題に書き換えるため、負の対数尤度を目的関数として用いる。最終的に、ロジスティック回帰モデルの学習で最小化する目的関数$\\hat{\\mathcal{L}}_{\\mathcal{D}}^{\\rm MLE}(\\bm{w})$は次式で表される。\n",
    "\n",
    ":::{admonition} 最尤推定による目的関数\n",
    ":class: important\n",
    "\\begin{align}\n",
    "\\hat{\\mathcal{L}}_{\\mathcal{D}}^{\\rm MLE}(\\bm{w}) = -\\log \\hat{L}_{\\mathcal{D}}(\\bm{w})\n",
    "= -\\sum_{i=1}^N \\log \\hat{l}_{\\bm{x}_i, y_i}(\\bm{w})\n",
    "\\end{align}\n",
    ":::\n",
    "\n",
    "また、学習時に$L_2$正則化を導入する場合、目的関数は、\n",
    "\\begin{align}\n",
    "\\hat{\\mathcal{L}}_{\\mathcal{D}}^{\\rm MAP}(\\bm{w}) &= -\\log \\hat{L}_{\\mathcal{D}}(\\bm{w}) + \\alpha \\|\\bm{w}\\|^2\n",
    "= -\\sum_{i=1}^N \\log \\hat{l}_{\\bm{x}_i, y_i}(\\bm{w}) + \\alpha \\|\\bm{w}\\|^2\n",
    "\\end{align}\n",
    "となる。ここで、$\\alpha$ ($\\alpha>0$) は$L_2$正則化の係数である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確率的勾配降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまでの議論により、ロジスティック回帰モデルの学習は、目的関数$\\hat{\\mathcal{L}}_{\\mathcal{D}}^{\\rm MLE}(\\bm{w})$を最小にするパラメータ$\\bm{w}^*$を求める問題に帰着した。回帰の場合は目的関数を最小にする解析解（閉じた解）を求めることができたが、ロジスティック回帰モデルの目的関数$\\hat{\\mathcal{L}}_{\\mathcal{D}}^{\\rm MLE}(\\bm{w})$は、パラメータ$\\bm{w}$に関して偏微分はできるが、その偏微分の値を$\\bm{0}$とする解析解を求めることができない。そこで、確率的勾配降下法で目的関数$\\hat{\\mathcal{L}}_{\\mathcal{D}}^{\\rm MLE}(\\bm{w})$を最小にするパラメータ$\\bm{w}^*$を求めることにする。\n",
    "\n",
    "ここで、最尤推定の目的関数$\\hat{\\mathcal{L}}_{\\mathcal{D}}^{\\rm MLE}(\\bm{w})$は、\n",
    "\\begin{align}\n",
    "\\hat{\\mathcal{L}}_{\\mathcal{D}}^{\\rm MLE}(\\bm{w}) &= -\\log \\hat{L}_{\\mathcal{D}}(\\bm{w})\n",
    "= \\sum_{i=1}^N - \\log \\hat{l}_{\\bm{x}_i, y_i}(\\bm{w})\n",
    "\\end{align}\n",
    "であり、事例毎の実数値の和に分解できるため、確率的勾配降下法を適用可能である。\n",
    "\n",
    "```{margin} ランダムに選んだ事例$(\\bm{x}, y)$\n",
    "前章の説明では、ランダムに選んだ訓練事例を$(\\bm{x}_i, y_i)$と事例番号$i$の添字付きで表記していたが、以降の導出で添字を使うと煩雑になるため、事例番号の添字は省略する。\n",
    "```\n",
    "\n",
    "確率的勾配降下法がランダムに訓練事例$(\\bm{x}, y) \\in \\mathcal{D}$を選んだとき、その勾配は$- \\nabla \\log \\hat{l}_{\\bm{x}, y}(\\bm{w})$であるから、確率的勾配降下法によるパラメータ$\\bm{w}$の更新式は、\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\bm{w}^{(t+1)} = \\bm{w}^{(t)} + \\eta_t \\nabla \\log \\hat{l}_{\\bm{x}, y}(\\bm{w}^{(t)})\n",
    "\\end{align}\n",
    "$$ (eq:sgd-gradient-update)\n",
    "\n",
    "となる。確率的勾配降下法でロジスティック回帰モデルのパラメータ推定を行うためには、訓練事例の勾配$\\nabla \\log \\hat{l}_{\\bm{x}, y}(\\bm{w})$、すなわち訓練事例の対数尤度$\\log \\hat{l}_{\\bm{x}, y}(\\bm{w})$の$\\bm{w}$に関する偏微分を求めればよい。\n",
    "\n",
    "学習事例の対数尤度は、式{eq}`eq:instance-likelihood`より、\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log \\hat{l}_{\\bm{x}, y}(\\bm{w}) &= \\log \\left(p^{y} (1-p)^{(1 - y)}\\right)\n",
    "= y \\log p + (1-y)\\log(1-p)\n",
    "\\end{align}\n",
    "$$ (eq:loss-binary-classification)\n",
    "\n",
    "と整理できる。ただし、式{eq}`eq:sigmoid`と{eq}`eq:definition-of-p`で表現されているように、$p$は$\\bm{w}$と$\\bm{x}$に依存していることに注意が必要である。\n",
    "\\begin{align}\n",
    "p &= \\sigma(a) = \\frac{1}{1 + e^{-a}} \\\\\n",
    "a &= \\bm{x}^\\top \\bm{w}\n",
    "\\end{align}\n",
    "\n",
    "合成関数の微分より、\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla \\log \\hat{l}_{\\bm{x}, y}(\\bm{w})\n",
    "= \\frac{\\partial \\left(\\log \\hat{l}_{\\bm{x}, y}(\\bm{w})\\right)}{\\partial \\bm{w}}\n",
    "&= \\frac{\\partial}{\\partial \\bm{w}} \\left\\{y \\log p + (1 - y) \\log (1-p) \\right\\} \\\\\n",
    "&= \\left( \\frac{y}{p} \\cdot \\frac{\\partial p}{\\partial \\bm{w}} + \\frac{1 - y}{1 - p} \\cdot (-1) \\cdot \\frac{\\partial p}{\\partial \\bm{w}} \\right) \\\\\n",
    "&= \\left(\\frac{y}{p} - \\frac{1 - y}{1 - p}\\right) \\cdot \\frac{\\partial p}{\\partial \\bm{w}} \\\\\n",
    "&= \\frac{(1-p) y - p(1 - y)}{p(1 - p)} \\cdot \\frac{\\partial p}{\\partial a} \\cdot \\frac{\\partial a}{\\partial \\bm{w}} \\\\\n",
    "&= \\frac{y - p y - p + p y}{p(1 - p)} \\cdot \\frac{\\partial p}{\\partial a} \\cdot \\frac{\\partial a}{\\partial \\bm{w}} \\\\\n",
    "&= \\frac{y - p}{p(1 - p)} \\cdot \\frac{\\partial p}{\\partial a} \\cdot \\frac{\\partial a}{\\partial \\bm{w}} \\\\\n",
    "\\end{align}\n",
    "$$ (eq:logress-gradient)\n",
    "\n",
    "$\\frac{\\partial p}{\\partial a}$はシグモイド関数$\\sigma(a)$の微分である。\n",
    "\\begin{align}\n",
    "\\frac{\\partial p}{\\partial a} = \\sigma'(a) &= \\frac{\\partial}{\\partial a}\\left(\\frac{1}{1 + e^{-a}}\\right) \\\\\n",
    "&= (-1) \\cdot \\frac{1}{\\left(1 + e^{-a}\\right)^2} \\cdot \\frac{\\partial}{\\partial a} \\left(1 + e^{-a}\\right) \\\\\n",
    "&= - \\frac{1}{\\left(1 + e^{-a}\\right)^2} \\cdot e^{-a} \\cdot (-1) \\\\\n",
    "&= \\frac{1}{1 + e^{-a}} \\cdot \\frac{e^{-a}}{1 + e^{-a}} \\\\\n",
    "&= \\sigma(a) \\left(1 - \\sigma(a)\\right) \\\\\n",
    "&= p (1 - p)\n",
    "\\end{align}\n",
    "\n",
    "また、$\\frac{\\partial a}{\\partial \\bm{w}}=\\bm{x}$であるから、式{eq}`eq:logress-gradient`は、\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla \\log \\hat{l}_{\\bm{x}, y}(\\bm{w})\n",
    "&= \\frac{y - p}{p(1 - p)} \\cdot \\frac{\\partial p}{\\partial a} \\cdot \\frac{\\partial a}{\\partial \\bm{w}} \\\\\n",
    "&= \\frac{y - p}{p(1 - p)} \\cdot \\{p (1 - p)\\} \\cdot \\bm{x} \\\\\n",
    "&= (y - p) \\bm{x} \\\\\n",
    "\\end{align}\n",
    "$$ (eq:grad-logress)\n",
    "\n",
    "これを、確率的勾配降下法の反復式{eq}`eq:sgd-gradient-update`に代入すると、\n",
    "\n",
    ":::{admonition} 確率的勾配降下法によるロジスティック回帰モデルのパラメータ更新式\n",
    ":class: important\n",
    "$$\n",
    "\\begin{align}\n",
    "\\bm{w}^{(t+1)} =  \\bm{w}^{(t)} + \\eta_t (y - p^{(t)}) \\bm{x}\n",
    "\\end{align}\n",
    "$$ (eq:logress-sgd-update)\n",
    ":::\n",
    "\n",
    "ただし、$p^{(t)}$は$t$回目の反復時のパラメータ$\\bm{w}^{(t)}$で事例$\\bm{x}$を分類したとき、その事例が正例$\\hat{y}=1$と予測される確率である。\n",
    "\\begin{align}\n",
    "p^{(t)} = \\sigma(\\bm{x}^\\top \\bm{w}^{(t)})\n",
    "\\end{align}\n",
    "\n",
    "確率的勾配降下法で回帰モデルのパラメータを求めるための反復式{eq}`eq:mra-sgd`と同様に、ある訓練事例における目的変数の真の値$y$とその確率推定値$p$の差を係数として、その事例の特徴ベクトル$\\bm{x}$をパラメータベクトルに足し込む形になっているのが興味深い。\n",
    "\n",
    "### 確率的勾配降下法の更新式の解釈\n",
    "\n",
    "式{eq}`eq:logress-sgd-update`の反復式の振る舞いを、学習事例が正例（$y=1$）のときと、負例（$y = 0$）に分けて考える。もし、学習事例が正例（$y=1$）のとき、パラメータの更新式は、\n",
    "\\begin{align}\n",
    "\\bm{w}^{(t+1)} &= \\bm{w}^{(t)} + \\eta_t (1 - p^{(t)}) \\bm{x}\n",
    "\\end{align}\n",
    "である。この更新後のパラメータ$\\bm{w}^{(t+1)}$で同じ事例$\\bm{x}$との内積を計算すると、\n",
    "\\begin{align}\n",
    "\\bm{x}^\\top\\bm{w}^{(t+1)} &= \\bm{x}^\\top\\bm{w}^{(t)} + \\eta_t (1 - p^{(t)}) \\underbrace{\\bm{x}^\\top\\bm{x}}_{\\geq 0} \\geq \\bm{x}^\\top\\bm{w}^{(t)}\n",
    "\\end{align}\n",
    "であるから、更新後のパラメータの内積$\\bm{x}^\\top\\bm{w}^{(t+1)}$は、更新前のパラメータの内積値$\\bm{x}^\\top\\bm{w}^{(t)}$以上にとなり、その変化量は$\\eta_t (1 - p^{(t)}) \\bm{x}^\\top\\bm{x}$である。ここで、学習率$\\eta_t$は学習事例に依存しない。また、$\\bm{x}^\\top\\bm{x}$もパラメータ$\\bm{w}^{(t)}$の値によらない定数と見なすと、内積の変化量は$(1-p^{(t)})$に比例することが分かる。いま、学習事例は正例（$y=1$）であるので、$(1-p^{(t)})$はこの事例に対して予測すべき確率$1$と、実際に計算された確率$p^{(t)}$との差を表している。したがって、モデルのパラメータ$\\bm{w}^{(t)}$が学習事例が正例であることをよく予測できている（$p^{(t)} \\approx 1$）ならば内積の変化量は小さくなる。逆に、モデルのパラメータ$\\bm{w}^{(t)}$が学習事例が正例であることを予測できていない場合（$p^{(t)} \\approx 0$）ならば、内積の変化量が大きくなるようにパラメータ$\\bm{w}^{(t)}$が更新される。\n",
    "\n",
    "続いて、学習事例が負例（$y=0$）のとき、パラメータの更新式は、\n",
    "\\begin{align}\n",
    "\\bm{w}^{(t+1)} = \\bm{w}^{(t)} + \\eta_t (0 - p^{(t)}) \\bm{x} = \\bm{w}^{(t)} - \\eta_t p^{(t)} \\bm{x}\n",
    "\\end{align}\n",
    "である。この更新後のパラメータ$\\bm{w}^{(t+1)}$で同じ事例$\\bm{x}$との内積を計算すると、\n",
    "\\begin{align}\n",
    "\\bm{x}^\\top\\bm{w}^{(t+1)} &= \\bm{x}^\\top\\bm{w}^{(t)} - \\eta_t p^{(t)} \\underbrace{\\bm{x}^\\top\\bm{x}}_{\\geq 0} \\leq \\bm{x}^\\top\\bm{w}^{(t)}\n",
    "\\end{align}\n",
    "であるから、更新後のパラメータの内積$\\bm{x}^\\top\\bm{w}^{(t+1)}$は、更新前のパラメータの内積値$\\bm{x}^\\top\\bm{w}^{(t)}$以下にとなり、その変化量は$\\eta_t p^{(t)} \\bm{x}^\\top\\bm{x}$である。先ほどの正例の議論と同様に、内積の変化量は$p^{(t)}$に比例することが分かる。いま、学習事例は負例（$y=0$）であるので、$p^{(t)}$はこの事例に対して予測すべき確率$0$と、実際に計算された確率$p^{(t)}$との差を表している。したがって、学習事例が負例であることをモデルのパラメータ$\\bm{w}^{(t)}$がよく予測できている（$p^{(t)} \\approx 0$）ならば内積の変化量は小さくなる。逆に、学習事例が負例であることをモデルのパラメータ$\\bm{w}^{(t)}$があまり予測できていない場合（$p^{(t)} \\approx 1$）、内積の変化量が大きくなるようにパラメータ$\\bm{w}^{(t)}$が更新される。\n",
    "\n",
    "### $L_2$正則化付きロジスティック回帰\n",
    "\n",
    "学習時に$L_2$正則化を導入する場合、目的関数は、\n",
    "\\begin{align}\n",
    "\\hat{\\mathcal{L}}_{\\mathcal{D}}^{\\rm MAP}(\\bm{w}) &= -\\log \\hat{L}_{\\mathcal{D}}(\\bm{w}) + \\alpha \\|\\bm{w}\\|^2 \\\\\n",
    "&= -\\sum_{i=1}^N \\log \\hat{l}_{\\bm{x}_i, y_i}(\\bm{w}) + \\alpha \\|\\bm{w}\\|^2 \\\\\n",
    "&= \\sum_{i=1}^N \\left(-\\log \\hat{l}_{\\bm{x}_i, y_i}(\\bm{w}) + \\frac{\\alpha}{N} \\|\\bm{w}\\|^2 \\right)\\\\\n",
    "\\end{align}\n",
    "と整理できるので、確率的勾配降下法を適用できる。確率的勾配降下法がランダムに選んだ学習事例を$(\\bm{x}, y)$とすると、その損失$l_{\\bm{x}, y}(\\bm{w})$は、\n",
    "\\begin{align}\n",
    "l_{\\bm{x}, y}(\\bm{w}) = -\\log \\hat{l}_{\\bm{x}, y}(\\bm{w}) + \\frac{\\alpha}{N} \\|\\bm{w}\\|^2\n",
    "\\end{align}\n",
    "である。この勾配$\\nabla l_{\\bm{x}, y}(\\bm{w})$を求めるために、$\\bm{w}$で偏微分すると、\n",
    "\\begin{align}\n",
    "\\nabla l_{\\bm{x}, y}(\\bm{w}) = \\frac{\\partial l_{\\bm{x}, y}(\\bm{w})}{\\partial \\bm{w}}\n",
    "&= \\frac{\\partial }{\\partial \\bm{w}}\\left(-\\log \\hat{l}_{\\bm{x}, y}(\\bm{w}) + \\frac{\\alpha}{N} \\|\\bm{w}\\|^2\\right) \\\\\n",
    "&= - \\frac{\\partial \\left(\\log \\hat{l}_{\\bm{x}, y}(\\bm{w})\\right)}{\\partial \\bm{w}} + \\frac{\\partial}{\\partial \\bm{w}} \\left(\\frac{\\alpha}{N} \\|\\bm{w}\\|^2\\right)\\\\\n",
    "&= - (y-p) \\bm{x} + \\frac{2\\alpha}{N} \\bm{w}\n",
    "\\end{align}\n",
    "\n",
    "これを、確率的勾配降下法の反復式に代入すると、\n",
    "\\begin{align}\n",
    "\\bm{w}^{(t+1)} &= \\bm{w}^{(t)} - \\eta_t \\left.\\frac{\\partial l_{\\bm{x}, y}(\\bm{w})}{\\partial \\bm{w}}\\right|_{\\bm{w} = \\bm{w}^{(t)}} \\\\\n",
    "&= \\bm{w}^{(t)} + \\eta_t \\left\\{(y - p) \\bm{x} - \\frac{2\\alpha}{N} \\bm{w}^{(t)}\\right\\} \\\\\n",
    "&= \\left(1 - \\frac{2\\alpha\\eta_t}{N}\\right) \\bm{w}^{(t)} + \\eta_t (y - p) \\bm{x}\n",
    "\\end{align}\n",
    "リッジ回帰と同様に、パラメータの重みを減衰させる係数$(1 - \\frac{2\\alpha\\eta_t}{N})$が現れる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価\n",
    "\n",
    "さて、学習によって獲得した二値分類モデルの性能（分類の正しさ）をどのように評価すればよいか。回帰のときは、学習で用いた目的関数、すなわち平均二乗残差を使って検証データやテストデータ上の性能を測定した。ロジスティック回帰モデルを最尤推定で学習する場合、尤度（もしくは対数尤度）を最大化していたので、検証データやテストデータ上で尤度を測定すればよい（実際に尤度が評価に用いられることもある）。ところが、尤度は人間にとって分かりやすい指標ではないため、二値分類ではもう少し分かりやすい評価尺度が用いられる。ここでは、二値分類モデルの評価尺度として、正解率、適合率、再現率、F1スコアを紹介する。\n",
    "\n",
    "これらの尺度を説明する前に、**真陽性**（TP: true positive）、**偽陽性**（FP: false positive）、**偽陰性**（FN: false negative）、**真陰性**（TN: true negative）の概念を理解する必要がある。モデルが正例と予測（$\\hat{y}=1$）した事例のうち、実際に正例（$y=1$）である事例を真陽性、実際には負例（$y=0$）である事例を疑陽性と呼ぶ。一方、モデルが負例と予測（$\\hat{y}=0$）した事例のうち、実際に負例（$y=0$）である事例を真陰性、実際には正例（$y=1$）である事例を偽陰性と呼ぶ。これらの概念を表に表すと、以下のようになる。\n",
    "\n",
    "<img src=\"fig/tpfpfntn.svg\" alt=\"混同行列\" width=\"480px\">\n",
    "\n",
    "ここで、真陽性、偽陽性、偽陰性、真陰性の事例数をそれぞれ、${\\rm TP}$, ${\\rm FP}$, ${\\rm FN}$, ${\\rm TN}$と書くことにすると、正解率$A$、適合率$P$、再現率$R$、F1スコア$F_1$は、\n",
    "\n",
    ":::{important}\n",
    "\\begin{gather}\n",
    "A = \\frac{\\mbox{システムの正しい予測事例数}}{\\mbox{評価した全事例数}} = \\frac{{\\rm TP} + {\\rm TN}}{{\\rm TP} + {\\rm TN} + {\\rm FP} + {\\rm FN}}\n",
    "\\end{gather}\n",
    "\\begin{gather}\n",
    "P = \\frac{\\mbox{システムが正しく正例と予測した事例数}}{\\mbox{システムが正例と予測した事例数}} = \\frac{{\\rm TP}}{{\\rm TP} + {\\rm FP}}\n",
    "\\end{gather}\n",
    "\\begin{gather}\n",
    "R = \\frac{\\mbox{システムが正しく正例と予測した事例数}}{\\mbox{評価データ中の正例の事例数}} = \\frac{{\\rm TP}}{{\\rm TP} + {\\rm FN}}\n",
    "\\end{gather}\n",
    "\\begin{gather}\n",
    "F_1 = \\frac{P \\times R}{\\frac{1}{2}(P + R)} = \\frac{2PR}{P + R}\n",
    "\\end{gather}\n",
    ":::\n",
    "\n",
    "この中で最も分かりやすい尺度は**正解率**（accuracy）であろう。正解率は、すべての評価事例の中でモデルが予測に成功した割合である。スパム判定の例では、届いたメールに対して、スパム判定の結果が正しかった割合を表す。ただし、評価データ中の正例と負例の割合が大きく偏っている場合、正解率は高くなりやすい。例えば、100件中1件しかスパムメールがやってこない状況では、すべてのメールを「スパムでない」と判定しても正解率が0.99となる。\n",
    "\n",
    "適合率と再現率はセットで理解する必要がある。**適合率**（precision）は、モデルが正例と予測した事例のうち、実際に正例である事例の割合である。スパム判定の例では、スパムと判定されたメールのうち、実際にスパムであるメールの割合である。スパム判定において適合率が低いと、本当はスパムではないメールがスパムフォルダに自動仕分けされてしまうことになる。一方、**再現率**（recall）は、実際に正例である事例のうち、モデルが正例として予測できる事例の割合である。スパム判定の例では、スパムメールの何割を自動的に認識できるかを表す。スパム判定において再現率が低いと、スパムメールがスパムフォルダに自動仕分けされず、新着メールとして頻繁に表示されてしまうことになる。\n",
    "\n",
    "一般に、適合率と再現率はトレードオフの関係にある。スパムメールの判定の適合率を高めるには、自信を持ってスパムメールと判定できるものだけスパムと判定し、あまり自信がない事例についてはスパムではないと判定すればよい。ところが、このようにして適合率を高めると、スパムメールと判定することに消極的となり、再現率が低下する。一方、再現率を高めるには、スパムメールと認定する基準を下げ、より多くのメールをスパムとして判定できるように調整すればよい。ところが、メールをスパムとして積極的に判定しすぎると、適合率が低下する。従って、モデルの性能を評価するときは、適合率と再現率の両方を測定することが望ましい。スパムメール判定では、スパムフォルダに自動的に仕分けされてしまったメールは読まれないことになってしまうため、再現率よりも適合率を重視すべきである。\n",
    "\n",
    "このように、適合率と再現率はトレードオフの関係にあるため、分類器の性能を測定するときにはこの両方の尺度の数値を見る必要がある。この適合率と再現率の調和平均をとったものが**F1スコア**である。F1スコアは異なるモデル間の性能を一つの評価尺度で比較できるので便利である。\n",
    "\n",
    "なお、病気などの検査では感度（sensitivity）と特異度（specificity）もよく用いられる。感度は再現率と同じ定義であり、実際に陽性となるべき事例をどの程度陽性として検出できたかを表す。特異度は負例に関する再現率であり、実際に陰性となるべき事例をどの程度陰性として検出できたかを表す。特異度（$S$）の定義は次式の通りである。\n",
    "\n",
    "\\begin{align}\n",
    "S &= \\frac{\\mbox{システムが正しく負例と予測した事例数}}{\\mbox{評価データ中の負例の事例数}} = \\frac{{\\rm TN}}{{\\rm TN} + {\\rm FP}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## スパムフィルタの構築\n",
    "\n",
    "### データのダウンロード\n",
    "\n",
    "[SMS Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)を用いて、英語のスパムフィルタを学習する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-08-01 06:20:41--  https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 203415 (199K) [application/x-httpd-php]\n",
      "Saving to: ‘smsspamcollection.zip’\n",
      "\n",
      "smsspamcollection.z 100%[===================>] 198.65K   249KB/s    in 0.8s    \n",
      "\n",
      "2021-08-01 06:20:43 (249 KB/s) - ‘smsspamcollection.zip’ saved [203415/203415]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ダウンロードしたファイルを解凍する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  smsspamcollection.zip\n",
      "  inflating: SMSSpamCollection       \n",
      "  inflating: readme                  \n"
     ]
    }
   ],
   "source": [
    "!unzip smsspamcollection.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データのファイル（SMSSpamCollection）は、１行１事例で書かれており、各事例はラベルとテキストのタブ区切り形式である。スパムではないメッセージは\"ham\"、スパムメッセージは\"spam\"としてラベル付けされている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "ham\tOk lar... Joking wif u oni...\n",
      "spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "ham\tU dun say so early hor... U c already then say...\n",
      "ham\tNah I don't think he goes to usf, he lives around here though\n",
      "spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n",
      "ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
      "ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "spam\tWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n"
     ]
    }
   ],
   "source": [
    "!head SMSSpamCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Windows版PythonなどでLinuxコマンドを利用できない場合\n",
    ":class: note, dropdown\n",
    "\n",
    "上の3つのコードセルでは、`!`を先頭に付けることでLinuxコマンドを呼び出している。ところが、Windows版のPython上で動作しているJupyterでは、Linuxコマンドを実行できない。その代替として、以下のPythonプログラムを実行すればよい。\n",
    "\n",
    "wgetコマンドの代わりに[SMS Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)をダウンロードするコード。\n",
    "\n",
    "```python\n",
    "import urllib.request\n",
    "filename, _ = urllib.request.urlretrieve(\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip',\n",
    "    'smsspamcollection.zip'\n",
    ")\n",
    "```\n",
    "\n",
    "zipコマンドの代わりにダウンロードしたファイルを解凍する。\n",
    "\n",
    "```python\n",
    "import zipfile\n",
    "with zipfile.ZipFile(filename, 'r') as fi:\n",
    "    fi.extractall('.')\n",
    "```\n",
    "\n",
    "解凍したファイルの先頭から10行を表示する。\n",
    "\n",
    "```python\n",
    "with open('SMSSpamCollection', encoding=\"utf-8\") as fi:\n",
    "    for n, line in enumerate(fi):\n",
    "        if n < 10:\n",
    "            print(line, end='')\n",
    "        else:\n",
    "            break\n",
    "```\n",
    "\n",
    "なお、Windows 10以降では[Windows Subsystem for Linux (WSL)](https://docs.microsoft.com/ja-jp/windows/wsl/install)をインストールすることで、UbuntuなどのLinux ディストリビューションを動作させ、その環境内でJupyterを立ち上げることができる。その場合、LinuxコマンドをJupyterのコードセルから直接呼び出すことが可能である。\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの読み込み\n",
    "\n",
    "このファイルから事例を読み込み、リストオブジェクト`D`に格納する。\n",
    "\n",
    ":::{admonition} Windows版Pythonで文字化けが発生する場合\n",
    ":class: note, dropdown\n",
    "\n",
    "Windows版のPythonでは、テキストファイルの読み書きの際の文字コードの規定値がCP932 (Shift_JIS) に設定されていることがある。ところが、SMSSpamCollectionというファイルの文字コードはUTF-8であるため、ファイルの内容を正常に読み込むことができない。その場合は、以下のプログラムで\n",
    "\n",
    "```python\n",
    "with open('SMSSpamCollection') as fi:\n",
    "```\n",
    "\n",
    "となっている箇所を、\n",
    "\n",
    "```python\n",
    "with open('SMSSpamCollection', encoding='utf8') as fi:\n",
    "```\n",
    "\n",
    "に変更すればよい。\n",
    "\n",
    "もしくは、`PYTHONUTF8`という環境変数に`1`をセットした状態でJupyterを立ち上げると、テキストファイルの読み書き時の文字コードの規定値がUTF-8となるので、プログラムを変更しなくてもSMSSpamCollectionをUTF-8のテキストファイルとして読み込むことができる（[PEP 540 -- Add a new UTF-8 Mode](https://www.python.org/dev/peps/pep-0540/)）。\n",
    "\n",
    "```bat\n",
    "set PYTHONUTF8=1\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def tokenize(s):\n",
    "    return [t.rstrip('.') for t in s.split(' ')]\n",
    "\n",
    "def vectorize(tokens):\n",
    "    return collections.Counter(tokens)\n",
    "\n",
    "def readiter(fi):\n",
    "    for line in fi:\n",
    "        fields = line.strip('\\n').split('\\t')\n",
    "        x = vectorize(tokenize(fields[1]))\n",
    "        y = fields[0]\n",
    "        yield x, y\n",
    "\n",
    "with open('SMSSpamCollection') as fi:\n",
    "    D = [d for d in readiter(fi)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`D`の各要素はメッセージ中に含まれる単語の出現頻度（`collections.Counter`オブジェクト）とラベルのタプルである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({'Even': 1,\n",
       "          'my': 1,\n",
       "          'brother': 1,\n",
       "          'is': 1,\n",
       "          'not': 1,\n",
       "          'like': 2,\n",
       "          'to': 1,\n",
       "          'speak': 1,\n",
       "          'with': 1,\n",
       "          'me': 2,\n",
       "          'They': 1,\n",
       "          'treat': 1,\n",
       "          'aids': 1,\n",
       "          'patent': 1}),\n",
       " 'ham')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)を用いて、このデータセットを訓練データ（90%）と評価データ（10%）に分割する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Dtrain, Dtest = train_test_split(D, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練データと評価データの事例数を確認しておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5016, 558)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Dtrain), len(Dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ形式の変換\n",
    "\n",
    "[sklearn.feature_extraction.DictVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html)と[sklearn.preprocessing.LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)を用いて、訓練データと評価データをscikit-learnが扱える行列形式に変換する。\n",
    "\n",
    "DictVectorizerは、特徴をキー、値をバリューとする辞書オブジェクトから特徴ベクトルに変換する。このとき、各特徴に$0$から始まるID番号を割り振っていくことで、文字列などで表現される特徴をベクトルの要素番号に対応づける。なお、テキストを単語を特徴とするベクトルで表現する場合、ベクトルの要素数はデータ中のすべての単語となるが、各事例のテキストは少数の単語で構成されるため、特徴ベクトルの多くの要素はゼロで、テキストに含まれている単語に対応する要素だけ非ゼロとなる。このように、要素の多くはゼロで、少数の要素が非ゼロであるベクトルは疎ベクトルと呼ばれる。DictVectorizerのデフォルトの動作では、辞書オブジェクトを[scipy.sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html)の疎ベクトル形式に変換する。\n",
    "\n",
    "[fit_transform](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer.fit_transform)関数は、\n",
    "特徴と要素番号の対応関係を更新しながら、辞書オブジェクトを疎ベクトルに変換する。[transform](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer.transform)関数は、特徴と要素番号の対応関係を更新せずに、辞書オブジェクトを疎ベクトルに変換する（対応関係が登録されていない特徴は無視される）。\n",
    "学習時に存在しなかった特徴を評価時に使うことができないため、前者を学習データに、後者を評価データに用いる。いずれの関数も、引数として辞書オブジェクトのリスト（事例のリスト）を与えると、返り値は事例の疎ベクトルをまとめた疎行列となる。\n",
    "\n",
    "LabelEncoderはラベルを整数値に変換する。各ラベルに$0$から始まるID番号を割り振っていくことで、文字列などで表現されるラベルをクラスの番号に対応づける。今回用いるデータは\"ham\"と\"spam\"の二つのラベルで構成されているため、これらのラベルに対して$0$または$1$が割り当てられる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "VX = DictVectorizer()\n",
    "VY = LabelEncoder()\n",
    "\n",
    "Xtrain = VX.fit_transform([d[0] for d in Dtrain])\n",
    "Ytrain = VY.fit_transform([d[1] for d in Dtrain])\n",
    "Xtest = VX.transform([d[0] for d in Dtest])\n",
    "Ytest = VY.transform([d[1] for d in Dtest])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練データの事例`Dtrain[10]`がどのように変換されたのか確認しておこう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({'I': 1,\n",
       "          'take': 1,\n",
       "          'it': 2,\n",
       "          'we': 3,\n",
       "          \"didn't\": 1,\n",
       "          'have': 2,\n",
       "          'the': 1,\n",
       "          'phone': 1,\n",
       "          'callon': 1,\n",
       "          'Friday': 1,\n",
       "          'Can': 1,\n",
       "          'assume': 1,\n",
       "          \"won't\": 1,\n",
       "          'this': 1,\n",
       "          'year': 1,\n",
       "          'now?': 1}),\n",
       " 'ham')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dtrain[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この事例の特徴ベクトルは疎ベクトルとして表現されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1831)\t1.0\n",
      "  (0, 2385)\t1.0\n",
      "  (0, 2769)\t1.0\n",
      "  (0, 5546)\t1.0\n",
      "  (0, 6110)\t1.0\n",
      "  (0, 6923)\t1.0\n",
      "  (0, 8101)\t2.0\n",
      "  (0, 8587)\t2.0\n",
      "  (0, 9821)\t1.0\n",
      "  (0, 10231)\t1.0\n",
      "  (0, 11832)\t1.0\n",
      "  (0, 11957)\t1.0\n",
      "  (0, 12014)\t1.0\n",
      "  (0, 12653)\t3.0\n",
      "  (0, 12862)\t1.0\n",
      "  (0, 13030)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "値が`3.0`となっているベクトルの列番号は`12653`である。`Dtrain[10]`の実行結果から、この特徴に対応する単語は\"we\"である。そこで、`12653`に対応づけられている特徴を調べると、\"we\"であることが確認できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VX.feature_names_[12653]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この事例に対応づけられたラベルのID番号は$0$である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(Ytrain[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LabelEncoderオブジェクトに格納されているラベルからID番号への対応付けを確認すると、\"ham\"が$0$番、\"spam\"が$1$番に割り当てられていることが分かる。つまり、$y=0$が\"ham\"、$y=1$が\"spam\"に対応付けられている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam'], dtype='<U4')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VY.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二値分類モデルの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[sklearn.linear_model.SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)は線形分類モデルを確率的勾配降下法で学習する。ロジスティック回帰モデルを学習するには、SGDClassifierの引数に`loss='log'`を指定する。[fit](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.fit)関数に訓練データを渡すことで、モデルのパラメータが学習される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(loss='log')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "model = SGDClassifier(loss='log')\n",
    "model.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分類器の適用・評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習した分類器のモデルを用い、評価データの先頭の事例を分類する。予測されたラベルのID番号は$0$なので、このメッセージは\"ham\"（スパムではない）と予測された。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(Xtest[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルがこの事例を\"ham\"および\"spam\"と予測する確率を表示する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99567905, 0.00432095]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(Xtest[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "評価データのすべての事例を使い、正解率を求める。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9695340501792115"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任意のテキストメッセージを分類モデルに適用する例。以下のメッセージはspamに分類された。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30330529, 0.69669471]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = \"Your account has been credited with 500 FREE Text Messages.\"\n",
    "model.predict_proba(VX.transform(vectorize(tokenize(msg))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルパラメータの確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習で求められたモデルのパラメータ（重み）は`coef_`メンバ変数で確認できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.37090795e-01, -1.76138728e-01, -7.74805758e-04, ...,\n",
       "         2.93789017e-02, -3.55600052e-01, -9.90864851e-04]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特徴を表す単語とその重みのタプルからなるリストを作成し、重みが小さい順に並べたものを変数`F`に格納する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = sorted(zip(VX.feature_names_, model.coef_[0]), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重みの値が負に大きいトップ20の単語を表示する。$y=0$は\"ham\"に対応するので、これらの単語を含むメッセージは\"ham\"と予測されやすくなる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('&lt;#&gt;', -1.3558829924814004),\n",
       " ('me', -1.303455143586944),\n",
       " ('So', -1.14067222707557),\n",
       " ('him', -1.0598045974397776),\n",
       " ('i', -1.0030856587407084),\n",
       " ('my', -0.989036736792116),\n",
       " ('?', -0.9782504941221924),\n",
       " ('good', -0.9565766623762151),\n",
       " ('I', -0.9376350998709289),\n",
       " ('Its', -0.8771757171170732),\n",
       " ('how', -0.8669824265651959),\n",
       " (':)', -0.8547153077528507),\n",
       " ('ask', -0.8455262058905626),\n",
       " ('Ok', -0.8390078922985393),\n",
       " (\"I'll\", -0.8386832325339574),\n",
       " ('', -0.8370907952027454),\n",
       " ('something', -0.8358732912496923),\n",
       " ('hi', -0.8223764866888945),\n",
       " (\"i'm\", -0.8157132793348036),\n",
       " ('&amp;', -0.8138253181689464)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重みの値が正に大きいトップ20の単語を表示する。$y=1$は\"spam\"に対応するので、これらの単語を含むメッセージは\"spam\"と予測されやすくなる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"let's\", 1.5410199592235803),\n",
       " ('-', 1.546029930147131),\n",
       " ('85233', 1.5646354767938881),\n",
       " ('FREE>Ringtone!Reply', 1.5646354767938881),\n",
       " ('To', 1.5765775613191408),\n",
       " ('Reply', 1.6688173510729016),\n",
       " ('84484', 1.7389287297155838),\n",
       " ('ringtoneking', 1.7389287297155838),\n",
       " ('146tf150p', 1.763641194218317),\n",
       " ('2/2', 1.763641194218317),\n",
       " ('text', 1.7664355866227701),\n",
       " ('won', 1.9273478586846209),\n",
       " ('service', 1.9282663075223407),\n",
       " ('&', 1.928861893331028),\n",
       " ('STOP', 2.0508768286731196),\n",
       " ('mobile', 2.059881177176412),\n",
       " ('now!', 2.112823595204192),\n",
       " ('txt', 2.1171488639279636),\n",
       " ('Txt', 2.119470386271705),\n",
       " ('Call', 2.374054661665122)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確認問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1) 確率的勾配降下法によるロジスティック回帰モデルの学習**\n",
    "\n",
    "確率的勾配降下法でロジスティック回帰モデルを学習するアルゴリズムを自前で実装せよ。学習データや評価データは自由に選んでよい（難しければ、前節で用いた[SMS Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)を用いよ）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2) 評価データ上での正解率**\n",
    "\n",
    "評価データ上で学習したモデルの正解率を測定せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3) 学習で求めたパラメータ**\n",
    "\n",
    "学習で求めたモデルのパラメータのうち、重みの絶対値が大きいものトップ20を、重みが正のものと負のものに分けて表示せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "[機械学習帳](https://chokkan.github.io/mlnote/) © Copyright 2020-2024 by [岡崎 直観 (Naoaki Okazaki)](https://www.chokkan.org/). この作品は<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\">クリエイティブ・コモンズ 表示 - 非営利 - 改変禁止 4.0 国際 ライセンス</a>の下に提供されています。 <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"><img alt=\"クリエイティブ・コモンズ・ライセンス\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png\" /></a>　ただし、作品中のコードセル部分は<a rel=\"license\" href=\"https://opensource.org/licenses/MIT\">MITライセンス</a>の下に提供されています。"
   ]
  }
 ],
 "metadata": {
  "@context": {
   "CreativeWork": "http://schema.org/CreativeWork",
   "Organization": "http://schema.org/Organization",
   "Person": "http://schema.org/Person",
   "author": "http://schema.org/author",
   "copyrightHolder": "http://schema.org/copyrightHolder",
   "copyrightYear": "http://schema.org/copyrightYear",
   "license": "http://schema.org/license",
   "name": "http://schema.org/name",
   "title": "http://schema.org/name",
   "url": "http://schema.org/url"
  },
  "@type": "CreativeWork",
  "author": [
   {
    "@type": "Person",
    "name": "Naoaki Okazaki",
    "url": "https://www.chokkan.org/"
   }
  ],
  "copyrightHolder": [
   {
    "@type": "Person",
    "name": "Naoaki Okazaki",
    "url": "https://www.chokkan.org/"
   }
  ],
  "copyrightYear": 2022,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "license": "https://creativecommons.org/licenses/by-nc-nd/4.0/deed.ja",
  "title": "機械学習帳"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
