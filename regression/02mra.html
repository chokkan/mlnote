
<!DOCTYPE html>

<html lang="ja">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>2. 重回帰 &#8212; 機械学習帳</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/.ipynb_checkpoints/custom-checkpoint.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://chokkan.github.io/mlnote/regression/02mra.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="検索" href="../search.html" />
    <link rel="next" title="3. モデル選択と正則化" href="03regularization.html" />
    <link rel="prev" title="1. 単回帰" href="01sra.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="ja">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-5R9M0GR7MW"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-5R9M0GR7MW');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">機械学習帳</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="この本を検索..." aria-label="この本を検索..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  回帰
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01sra.html">
   1. 単回帰
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   2. 重回帰
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03regularization.html">
   3. モデル選択と正則化
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04sgd.html">
   4. 勾配法によるパラメータ推定
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  分類
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../classification/01binary.html">
   5. 線形二値分類
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../classification/02multi.html">
   6. 線形多クラス分類
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../classification/03nn.html">
   7. ニューラルネットワーク (1)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../classification/04nntrain.html">
   8. ニューラルネットワーク (2)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../classification/05svm.html">
   9. サポートベクトルマシン
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  教師無し学習
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../unsupervised/01kmeans.html">
   10. 非階層的クラスタリング
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../unsupervised/02hac.html">
   11. 階層的クラスタリング
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../unsupervised/03pca.html">
   12. 主成分分析 (1)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../unsupervised/04pca2.html">
   13. 主成分分析 (2)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  付録
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../notation.html">
   表記
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reference.html">
   参考文献・謝辞
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="ナビゲーションを切り替え" aria-controls="site-navigation"
                title="ナビゲーションを切り替え" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="このページをダウンロード"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/regression/02mra.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="ソースファイルをダウンロード" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="PDFに印刷"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/chokkan/mlnote"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="ソースリポジトリ"><i
                    class="fab fa-github"></i>リポジトリ</button></a>
        <a class="issues-button"
            href="https://github.com/chokkan/mlnote/issues/new?title=Issue%20on%20page%20%2Fregression/02mra.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="問題を開く"><i class="fas fa-lightbulb"></i>未解決の問題</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="全画面モード"
        title="全画面モード"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/chokkan/mlnote/master?urlpath=lab/tree/regression/02mra.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="発売 Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/chokkan/mlnote/blob/master/regression/02mra.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="発売 Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="発売 Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> 目次
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   2.1. 重回帰
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   2.2. 表記法の定義
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     2.2.1. 単回帰を表現する
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   2.3. 目的関数
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mra-minimization">
   2.4. 目的関数の最小化
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id7">
   2.5. 目的関数の微分（別の導出）
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id8">
   2.6. 目的関数は凸関数
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id9">
   2.7. 重回帰の性質
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id11">
     2.7.1. 残差の和および平均は
     <span class="math notranslate nohighlight">
      \(0\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hat-y-y">
     2.7.2. 目的変数の推定値
     <span class="math notranslate nohighlight">
      \(\hat{y}\)
     </span>
     の平均値は観測値
     <span class="math notranslate nohighlight">
      \(y\)
     </span>
     の平均に等しい
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id12">
     2.7.3. 観測データの重心は回帰平面上に存在する
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id13">
     2.7.4. 各説明変数と残差には相関がない
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id14">
     2.7.5. 目的変数の推定値と残差には相関がない
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id15">
   2.8. 決定係数
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id16">
   2.9. 重回帰の実施例
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#numpy-polyfit">
     2.9.1. numpy.polyfitを用いる例
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sklearn-linear-model-linearregression">
     2.9.2. sklearn.linear_model.LinearRegressionを用いる例
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id17">
   2.10. 確認問題
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="id1">
<h1><span class="section-number">2. </span>重回帰<a class="headerlink" href="#id1" title="このヘッドラインへのパーマリンク">¶</a></h1>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip install japanize-matplotlib
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: japanize-matplotlib in /usr/local/lib/python3.8/dist-packages (1.1.3)
Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from japanize-matplotlib) (3.4.2)
Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib-&gt;japanize-matplotlib) (1.3.1)
Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib-&gt;japanize-matplotlib) (0.10.0)
Requirement already satisfied: pillow&gt;=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib-&gt;japanize-matplotlib) (8.3.1)
Requirement already satisfied: pyparsing&gt;=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib-&gt;japanize-matplotlib) (2.4.7)
Requirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib-&gt;japanize-matplotlib) (2.8.1)
Requirement already satisfied: numpy&gt;=1.16 in /home/okazaki/.local/lib/python3.8/site-packages (from matplotlib-&gt;japanize-matplotlib) (1.19.5)
Requirement already satisfied: six in /home/okazaki/.local/lib/python3.8/site-packages (from cycler&gt;=0.10-&gt;matplotlib-&gt;japanize-matplotlib) (1.15.0)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">japanize_matplotlib</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id2">
<h2><span class="section-number">2.1. </span>重回帰<a class="headerlink" href="#id2" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>これまで、以下のデータ<span class="math notranslate nohighlight">\(\mathcal{D}_s\)</span>を例として単回帰を説明した。</p>
<div class="amsmath math notranslate nohighlight" id="equation-10ba36d0-3801-4318-a58c-b992385daa79">
<span class="eqno">(2.1)<a class="headerlink" href="#equation-10ba36d0-3801-4318-a58c-b992385daa79" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\mathcal{D}_s = \left\{(x_i, y_i)\right\}_{i=1}^{4} = \left\{(1, 3), (3, 6), (6, 5), (8, 7)\right\}
\end{align}\]</div>
<p>また、求めた回帰直線をデータ点とともにプロットしたものを以下に示す。</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mf">0.431</span><span class="p">,</span> <span class="mf">3.310</span> 
<span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">7</span><span class="p">]])</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">D</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">D</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span><span class="p">,</span> <span class="s1">&#39;tab:red&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s1">x + </span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">D</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">row</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">([</span><span class="n">x</span><span class="p">],</span> <span class="nb">min</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">),</span> <span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/02mra_4_0.png" src="../_images/02mra_4_0.png" />
</div>
</div>
<p>残差の平均二乗和が最小になるような直線を引いたはずであるが、残差を完全に無くすことはできなかった（決定係数を求めてみると<span class="math notranslate nohighlight">\(0.616\)</span>であり、あまり高くない）。プロットされている点を眺めると、３次関数を当てはめる方が向いているように思われる。そこで、１次関数よりも表現力が高いモデルとして、<span class="math notranslate nohighlight">\(d\)</span>次の多項式の一般形、</p>
<div class="amsmath math notranslate nohighlight" id="equation-b53c1a65-4a32-4bba-86c1-70f0eadfa7b2">
<span class="eqno">(2.2)<a class="headerlink" href="#equation-b53c1a65-4a32-4bba-86c1-70f0eadfa7b2" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\hat{y} = w_0 + w_1 x + w_2 x^2 + \dots + w_d x^d
\end{align}\]</div>
<p>にフィッティングすることを考えたい。ここで、<span class="math notranslate nohighlight">\(w_0, w_1, w_2, \dots, w_d \in \mathbb{R}\)</span>はモデルの<strong>パラメータ</strong>である。</p>
<p>なお、<span class="math notranslate nohighlight">\(\hat{y}\)</span>は重み<span class="math notranslate nohighlight">\(w_0, w_1, w_2, \dots, w_d\)</span>による説明変数<span class="math notranslate nohighlight">\(1, x, x^2, \dots x^d\)</span>の線形重み付き和と考えることができる。このモデルは多項式の一般形ではあるが、<span class="math notranslate nohighlight">\(\hat{y}\)</span>は<span class="math notranslate nohighlight">\(w_i\)</span>と<span class="math notranslate nohighlight">\(x^i\)</span>の線形結合で表されているため、<strong>線形モデル</strong>と見なすことができる。ただし、説明変数が２個以上であるので、単回帰を重回帰に拡張する必要がある。</p>
<p>一般的に、<span class="math notranslate nohighlight">\(d\)</span>個の説明変数<span class="math notranslate nohighlight">\(x_1, x_2, \dots, x_d\)</span>に対して、線形モデル、すなわち<span class="math notranslate nohighlight">\(d\)</span>個の重み<span class="math notranslate nohighlight">\(w_1, w_2, \dots, w_d\)</span>の線形結合で目的変数の予測値<span class="math notranslate nohighlight">\(\hat{y}\)</span>を計算することを<strong>線形回帰</strong>と呼ぶ。</p>
<div class="amsmath math notranslate nohighlight" id="equation-fa769f22-6a51-445f-8031-2451f9936ede">
<span class="eqno">(2.3)<a class="headerlink" href="#equation-fa769f22-6a51-445f-8031-2451f9936ede" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\hat{y} = w_1 x_1 + w_2 x_2 + \dots + w_d x_d
\end{align}\]</div>
<p>ゆえに、<span class="math notranslate nohighlight">\((d-1)\)</span>次の多項式へのフィッティングは、<span class="math notranslate nohighlight">\(d\)</span>個の説明変数<span class="math notranslate nohighlight">\(x_1 = 1\)</span>, <span class="math notranslate nohighlight">\(x_2 = x\)</span>, <span class="math notranslate nohighlight">\(x_3 = x^2\)</span>, <span class="math notranslate nohighlight">\(\dots\)</span>, <span class="math notranslate nohighlight">\(x_d = x^{d-1}\)</span>による線形回帰と見なすことができる。下図は、重回帰でデータ<span class="math notranslate nohighlight">\(\mathcal{D}_s\)</span>に３次関数を当てはめた結果である。本章の内容をマスターすると、<span class="math notranslate nohighlight">\(4\)</span>個の説明変数<span class="math notranslate nohighlight">\(1, x, x^2, x^3\)</span>に対して、その重み（<span class="math notranslate nohighlight">\(-1.23, 5.41, -1.27, 0.09\)</span>）を求めることができる。</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mf">0.431</span><span class="p">,</span> <span class="mf">3.310</span> 
<span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">7</span><span class="p">]])</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">D</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">D</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">0.09</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">-</span> <span class="mf">1.27</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mf">5.41</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="mf">1.23</span><span class="p">,</span> <span class="s1">&#39;tab:red&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$0.09 x^3 - 1.27 x^2 + 5.41 x - 1.23$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/02mra_6_0.png" src="../_images/02mra_6_0.png" />
</div>
</div>
<p>回帰分析において、説明変数を何個用意するか、それぞれの説明変数をどのように定義するかは任意である。たとえば、東京におけるアイスクリームへの支出額を予測するために、最高気温だけでなく、予測したい月の数字やアイスクリームの平均価格も有力な説明変数になり得るかもしれない。また、最高気温の平方根や、最高気温と月の積など、元の説明変数に非線形変換を適用した説明変数を導入してもよい。分析者が非線形変換の方法を明示的に設計する必要があるが、説明変数の組み合わせを考慮した非線形な回帰分析も、線形回帰の枠組みで行うことができる。</p>
<p><img alt="non-linear" src="../_images/non-linear.png" /></p>
</div>
<div class="section" id="id3">
<h2><span class="section-number">2.2. </span>表記法の定義<a class="headerlink" href="#id3" title="このヘッドラインへのパーマリンク">¶</a></h2>
<div class="margin sidebar">
<p class="sidebar-title">ベクトル</p>
<p>この資料では、ベクトルを小文字の太字で表す。特に断りがない限り、ベクトルは要素を縦に並べた<strong>列ベクトル</strong>であるとする。例えば、<span class="math notranslate nohighlight">\(1, 0, -1\)</span>を要素とする<span class="math notranslate nohighlight">\(3\)</span>次元ベクトル<span class="math notranslate nohighlight">\(\pmb{v} \in \mathbb{R}^3\)</span>を、</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\pmb{v} = \begin{pmatrix}1 \\ 0 \\ -1\end{pmatrix}
\end{split}\]</div>
<p>と書く。なお、横書きの文書で列ベクトルを書くとスペースを消費してしまうので、転置の演算子（<span class="math notranslate nohighlight">\(\top\)</span>）を用いて、</p>
<div class="math notranslate nohighlight">
\[
\pmb{v} = \begin{pmatrix}1 &amp; 0 &amp; -1\end{pmatrix}^\top
\]</div>
<p>と書くことがある。</p>
</div>
<p>重回帰の目的関数を一般的に記述するため、ベクトルによる表記を導入する。<span class="math notranslate nohighlight">\(d\)</span>個の説明変数<span class="math notranslate nohighlight">\(x_1, x_2, \dots, x_d\)</span>をベクトル<span class="math notranslate nohighlight">\(\pmb{x} \in \mathbb{R}^d\)</span>でまとめて表現する。</p>
<div class="amsmath math notranslate nohighlight" id="equation-345dddf1-b613-4304-a6b8-4940c0702769">
<span class="eqno">(2.4)<a class="headerlink" href="#equation-345dddf1-b613-4304-a6b8-4940c0702769" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\pmb{x} = \begin{pmatrix}
x_1 \\ x_2 \\ \dots \\ x_d
\end{pmatrix}
\end{align}\]</div>
<p>また、<span class="math notranslate nohighlight">\(d\)</span>個のパラメータ（重み）<span class="math notranslate nohighlight">\(w_1, w_2, \dots, w_d\)</span>をベクトル<span class="math notranslate nohighlight">\(\pmb{w} \in \mathbb{R}^d\)</span>でまとめて表現する。</p>
<div class="amsmath math notranslate nohighlight" id="equation-29924cc0-993e-487d-a09e-cb031d3354ef">
<span class="eqno">(2.5)<a class="headerlink" href="#equation-29924cc0-993e-487d-a09e-cb031d3354ef" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\pmb{w} = \begin{pmatrix}
w_1 \\ w_2 \\ \dots \\ w_d
\end{pmatrix}
\end{align}\]</div>
<div class="margin sidebar">
<p class="sidebar-title">ベクトルの内積</p>
<p><span class="math notranslate nohighlight">\(d\)</span>次元ベクトル、</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\pmb{u} &amp;= \begin{pmatrix}u_1 &amp; u_2 &amp; \dots &amp; u_d\end{pmatrix}^\top \\
\pmb{v} &amp;= \begin{pmatrix}v_1 &amp; v_2 &amp; \dots &amp; v_d\end{pmatrix}^\top
\end{align*}\]</div>
<p>があるとき、その内積は、</p>
<div class="math notranslate nohighlight">
\[
\pmb{u} \cdot \pmb{v} = u_1 v_1 + u_2 v_2 + \dots + u_d v_d
\]</div>
<p>である。これは、転置を使って、</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\pmb{u} \cdot \pmb{v} &amp;= \pmb{u}^\top \pmb{v} \\
&amp;= \begin{pmatrix}u_1 &amp; u_2 &amp; \dots &amp; u_d\end{pmatrix} \begin{pmatrix}v_1 \\ v_2 \\ \dots \\ v_d\end{pmatrix}
\end{align*}\]</div>
<p>と書ける。</p>
</div>
<p>すると、重回帰モデルによる目的変数の予測値<span class="math notranslate nohighlight">\(\hat{y}\)</span>は、ベクトル<span class="math notranslate nohighlight">\(\pmb{x}\)</span>と<span class="math notranslate nohighlight">\(\pmb{w}\)</span>の内積として記述できる。</p>
<div class="amsmath math notranslate nohighlight" id="equation-793c7b2f-7473-4934-bc66-29d5516a87e5">
<span class="eqno">(2.6)<a class="headerlink" href="#equation-793c7b2f-7473-4934-bc66-29d5516a87e5" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\hat{y} = \pmb{x}^\top\pmb{w} = w_1 x_1 + w_2 x_2 + \dots + w_d x_d
\end{align}\]</div>
<p>単回帰のときと同様に、説明変数と目的変数の組を<strong>事例</strong>として表現する。単回帰の場合と異なるのは、説明変数がスカラー<span class="math notranslate nohighlight">\(x\)</span>からベクトル<span class="math notranslate nohighlight">\(\pmb{x}\)</span>に拡張されたことである。
<span class="math notranslate nohighlight">\(1\)</span>番目の事例を<span class="math notranslate nohighlight">\((\pmb{x}_1, y_1)\)</span>、<span class="math notranslate nohighlight">\(2\)</span>番目の事例を<span class="math notranslate nohighlight">\((\pmb{x}_2, y_2)\)</span>、<span class="math notranslate nohighlight">\(i\)</span>番目の事例を<span class="math notranslate nohighlight">\((\pmb{x}_i, y_i)\)</span>と表すことにすると、<span class="math notranslate nohighlight">\(N\)</span>個の事例からなるデータ<span class="math notranslate nohighlight">\(\mathcal{D}\)</span>は次のように表される。</p>
<div class="amsmath math notranslate nohighlight" id="equation-a36eb8d9-9b35-430e-939c-9917874df51d">
<span class="eqno">(2.7)<a class="headerlink" href="#equation-a36eb8d9-9b35-430e-939c-9917874df51d" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\mathcal{D} = \left\{(\pmb{x}_1, y_1), (\pmb{x}_2, y_2), \dots, (\pmb{x}_N, y_N)\right\} = \left\{(\pmb{x}_i, y_i)\right\}_{i=1}^{N}
\end{align}\]</div>
<div class="margin sidebar">
<p class="sidebar-title">行列</p>
<p>この資料では、行列を大文字の太字で表す。<span class="math notranslate nohighlight">\(\pmb{X}\)</span>が実数値を要素とする<span class="math notranslate nohighlight">\(N\)</span>行<span class="math notranslate nohighlight">\(M\)</span>列の行列（<span class="math notranslate nohighlight">\(N \times M\)</span>の行列とも呼ばれる）であることを、<span class="math notranslate nohighlight">\(\pmb{X} \in \mathbb{R}^{N \times M}\)</span>と書く。行列<span class="math notranslate nohighlight">\(\pmb{X}\)</span>の<span class="math notranslate nohighlight">\(i\)</span>行<span class="math notranslate nohighlight">\(j\)</span>列の要素を<span class="math notranslate nohighlight">\(X_{i,j}\)</span>と表現する。</p>
<p>例えば、<span class="math notranslate nohighlight">\(2 \times 3\)</span>の行列<span class="math notranslate nohighlight">\(\pmb{X}\)</span>に関して、</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\pmb{X} = \begin{pmatrix}
1 &amp; 3 &amp; 5 \\
6 &amp; 4 &amp; 2 \\
\end{pmatrix}
\end{align*}\]</div>
<p><span class="math notranslate nohighlight">\(X_{1,2} = 3\)</span>、<span class="math notranslate nohighlight">\(X_{2,3} = 2\)</span>である。</p>
</div>
<p>さらに、<span class="math notranslate nohighlight">\(N\)</span>個の事例や目的変数を行列やベクトルで記述する表記法も導入しておく。説明変数のベクトル<span class="math notranslate nohighlight">\(\pmb{x}_i\)</span>を行ベクトルとして縦に<span class="math notranslate nohighlight">\(N\)</span>個並べた行列<span class="math notranslate nohighlight">\(\pmb{X} \in \mathbb{R}^{N \times d}\)</span>を定義する。この行列<span class="math notranslate nohighlight">\(\pmb{X}\)</span>は<strong>計画行列</strong>（design matrix）と呼ばれ、それぞれの事例の<span class="math notranslate nohighlight">\(d\)</span>個の説明変数を水平方向に<span class="math notranslate nohighlight">\(d\)</span>個並べ、<span class="math notranslate nohighlight">\(N\)</span>個の事例を垂直方向に<span class="math notranslate nohighlight">\(N\)</span>個並べたものである。</p>
<div class="amsmath math notranslate nohighlight" id="equation-dfe9f437-570d-4428-8d08-228af809e2c3">
<span class="eqno">(2.8)<a class="headerlink" href="#equation-dfe9f437-570d-4428-8d08-228af809e2c3" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\pmb{X} = \begin{pmatrix}
\pmb{x}_1^\top \\
\pmb{x}_2^\top \\
\dots \\
\pmb{x}_N^\top \\
\end{pmatrix}
= \begin{pmatrix}
X_{1,1} &amp; X_{1,2} &amp; \dots &amp; X_{1,d} \\
X_{2,1} &amp; X_{2,2} &amp; \dots &amp; X_{2,d} \\
\dots &amp; \dots &amp; \dots &amp; \dots \\
X_{N,1} &amp; X_{N,2} &amp; \dots &amp; X_{N,d}
\end{pmatrix}
\end{align}\]</div>
<p>ここで、<span class="math notranslate nohighlight">\(X_{i,j}\)</span>は行列<span class="math notranslate nohighlight">\(\pmb{X}\)</span>の<span class="math notranslate nohighlight">\(i\)</span>行<span class="math notranslate nohighlight">\(j\)</span>列の要素を表す。</p>
<p>また、目的変数<span class="math notranslate nohighlight">\(y_i\)</span>を縦に<span class="math notranslate nohighlight">\(N\)</span>個並べたベクトル<span class="math notranslate nohighlight">\(\pmb{y} \in \mathbb{R}^N\)</span>を定義する。</p>
<div class="amsmath math notranslate nohighlight" id="equation-ee4c4ba4-11a2-4f1e-9b04-249f6f6610ae">
<span class="eqno">(2.9)<a class="headerlink" href="#equation-ee4c4ba4-11a2-4f1e-9b04-249f6f6610ae" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\pmb{y} = \begin{pmatrix}
y_1 \\ y_2 \\ \dots \\ y_N
\end{pmatrix}
\end{align}\]</div>
<div class="margin sidebar">
<p class="sidebar-title">行列・ベクトル積</p>
<p><span class="math notranslate nohighlight">\(N \times d\)</span>の行列<span class="math notranslate nohighlight">\(\pmb{X}\)</span>と<span class="math notranslate nohighlight">\(d\)</span>行のベクトル<span class="math notranslate nohighlight">\(\pmb{w}\)</span>の積を計算した結果は、<span class="math notranslate nohighlight">\(N\)</span>行のベクトルとなる。</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp; \pmb{X}\pmb{w} \\
&amp;= \begin{pmatrix}
X_{1,1} &amp; \dots &amp; X_{1,d} \\
\dots &amp; \dots &amp; \dots \\
X_{N,1} &amp; \dots &amp; X_{N,d} \\
\end{pmatrix}
\begin{pmatrix}
w_1 \\ \dots \\ w_d
\end{pmatrix} \\
&amp;= \begin{pmatrix}
X_{1,1} w_1 + \dots + X_{1,d} w_d \\
\dots \\
X_{N,1} w_1 + \dots + X_{N,d} w_d \\
\end{pmatrix}
\end{align*}\]</div>
<p>例えば、<span class="math notranslate nohighlight">\(2 \times 3\)</span>の行列<span class="math notranslate nohighlight">\(\pmb{X}\)</span>と<span class="math notranslate nohighlight">\(3\)</span>行のベクトル<span class="math notranslate nohighlight">\(\pmb{w}\)</span>があるとき、</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\pmb{X} = \begin{pmatrix}
1 &amp; 3 &amp; 5 \\
6 &amp; 4 &amp; 2 \\
\end{pmatrix}, 
\pmb{w} = \begin{pmatrix}
2 \\
-2 \\
3\\
\end{pmatrix}
\end{align*}\]</div>
<p>行列<span class="math notranslate nohighlight">\(\pmb{X}\)</span>とベクトル<span class="math notranslate nohighlight">\(\pmb{w}\)</span>の積は、</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\pmb{X}\pmb{w}
&amp;= \begin{pmatrix}
1 &amp; 3 &amp; 5 \\
6 &amp; 4 &amp; 2 \\
\end{pmatrix} \begin{pmatrix}
2 \\
-2 \\
3\\
\end{pmatrix} \\
&amp;= \begin{pmatrix}
1 \times 2 + 3 \times (-2) + 5 \times 3 \\
6 \times 2 + 4 \times (-2) + 2 \times 3 \\
\end{pmatrix} \\
&amp;= \begin{pmatrix}
11 \\
10 \\
\end{pmatrix}
\end{align*}\]</div>
</div>
<p>目的変数の予測値<span class="math notranslate nohighlight">\(\hat{y}_i\)</span>を縦に<span class="math notranslate nohighlight">\(N\)</span>個並べ、ベクトル<span class="math notranslate nohighlight">\(\hat{\pmb{y}} \in \mathbb{R}^N\)</span>と書くことにすると、その予測値は計画行列とパラメータベクトルの積で求めることができる。</p>
<div class="amsmath math notranslate nohighlight" id="equation-1ff562c5-5e3e-452b-b5ad-06e5363b7f48">
<span class="eqno">(2.10)<a class="headerlink" href="#equation-1ff562c5-5e3e-452b-b5ad-06e5363b7f48" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\hat{\pmb{y}} = \begin{pmatrix}
\hat{y}_1 \\ \hat{y}_2 \\ \dots \\ \hat{y}_N
\end{pmatrix} = \begin{pmatrix}
\pmb{x}_1^\top \pmb{w} \\ \pmb{x}_2^\top \pmb{w} \\ \dots \\ \pmb{x}_N^\top \pmb{w}
\end{pmatrix}
= \pmb{X} \pmb{w}
\end{align}\]</div>
<div class="section" id="id4">
<h3><span class="section-number">2.2.1. </span>単回帰を表現する<a class="headerlink" href="#id4" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>これまでに定義した表記法で単回帰を記述できることを確認しておこう。<span class="math notranslate nohighlight">\(d=2\)</span>として説明変数とパラメータベクトルを以下のように定義する。</p>
<div class="math notranslate nohighlight" id="equation-eq-simple-linear-regression">
<span class="eqno">(2.11)<a class="headerlink" href="#equation-eq-simple-linear-regression" title="この数式へのパーマリンク">¶</a></span>\[\begin{split}
\begin{align}
\pmb{x} = \begin{pmatrix}
1 \\ x
\end{pmatrix},
\pmb{w} = \begin{pmatrix}
b \\ a
\end{pmatrix}
\end{align}
\end{split}\]</div>
<p>目的変数の予測値<span class="math notranslate nohighlight">\(\hat{y}\)</span>を計算する式を展開すると、単回帰モデルが得られる。</p>
<div class="amsmath math notranslate nohighlight" id="equation-11bb0901-f34d-432f-b83a-0fa9915b0027">
<span class="eqno">(2.12)<a class="headerlink" href="#equation-11bb0901-f34d-432f-b83a-0fa9915b0027" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\hat{y} = \pmb{x}^\top\pmb{w} = b + xa = ax + b
\end{align}\]</div>
<p>冒頭のグラフの例（<span class="math notranslate nohighlight">\(N=4\)</span>）の学習データは次のように表現される。</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathcal{D}_s = \left\{(\pmb{x}_i, y_i)\right\}_{i=1}^{4} = \left\{\left(\begin{pmatrix}1 &amp; 1\end{pmatrix}^\top, 3\right), \left(\begin{pmatrix}1 &amp; 3\end{pmatrix}^\top, 6\right), \left(\begin{pmatrix}1 &amp; 6\end{pmatrix}^\top, 5\right), \left(\begin{pmatrix}1 &amp; 8\end{pmatrix}^\top, 7\right)\right\}
\end{align*}\]</div>
<p>計画行列<span class="math notranslate nohighlight">\(\pmb{X} \in \mathbb{R}^{4 \times 2}\)</span>と目的変数ベクトル<span class="math notranslate nohighlight">\(\pmb{y} \in \mathbb{R}^4\)</span>は、それぞれ、</p>
<div class="amsmath math notranslate nohighlight" id="equation-960cb8cb-5a57-41be-8747-3bbb5cb9f193">
<span class="eqno">(2.13)<a class="headerlink" href="#equation-960cb8cb-5a57-41be-8747-3bbb5cb9f193" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\pmb{X} = \begin{pmatrix}
1 &amp; 1 \\
1 &amp; 3 \\
1 &amp; 6 \\
1 &amp; 8
\end{pmatrix},
\pmb{y} = \begin{pmatrix}
3 \\ 6 \\ 5 \\ 7
\end{pmatrix}
\end{align}\]</div>
<p>目的変数の予測ベクトル<span class="math notranslate nohighlight">\(\hat{\pmb{y}}\)</span>は行列<span class="math notranslate nohighlight">\(W\)</span>とベクトル<span class="math notranslate nohighlight">\(\pmb{w}\)</span>の積で求めることができる。</p>
<div class="amsmath math notranslate nohighlight" id="equation-03e81e1e-d01c-4cfc-96c4-135722c37daf">
<span class="eqno">(2.14)<a class="headerlink" href="#equation-03e81e1e-d01c-4cfc-96c4-135722c37daf" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\hat{\pmb{y}}
= \pmb{X} \pmb{w}
= \begin{pmatrix}
1 &amp; 1 \\
1 &amp; 3 \\
1 &amp; 6 \\
1 &amp; 8
\end{pmatrix}
\begin{pmatrix}
b \\ a
\end{pmatrix}
= \begin{pmatrix}
a + b \\
3a + b \\
6a + b \\
8a + b 
\end{pmatrix}
\end{align}\]</div>
</div>
</div>
<div class="section" id="id5">
<h2><span class="section-number">2.3. </span>目的関数<a class="headerlink" href="#id5" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>単回帰のときと同様に、ある事例の目的変数の真の値<span class="math notranslate nohighlight">\(y_i\)</span>と推定値<span class="math notranslate nohighlight">\(\hat{y}_i\)</span>の差を残差<span class="math notranslate nohighlight">\(\epsilon_i\)</span>とする。</p>
<div class="amsmath math notranslate nohighlight" id="equation-41416c99-bebd-422d-9f80-9bc9ead18fd4">
<span class="eqno">(2.15)<a class="headerlink" href="#equation-41416c99-bebd-422d-9f80-9bc9ead18fd4" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\epsilon_i = y_i - \hat{y}_i = y_i - \pmb{x}_i^\top\pmb{w}
\end{align}\]</div>
<p>学習データにおける平均二乗残差は、</p>
<div class="amsmath math notranslate nohighlight" id="equation-02be55b2-5d9e-459c-bca5-8c0d265f94b4">
<span class="eqno">(2.16)<a class="headerlink" href="#equation-02be55b2-5d9e-459c-bca5-8c0d265f94b4" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\hat{L} = \frac{1}{N} \sum_{i=1}^{N} \epsilon_i^2 = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 = \frac{1}{N} \sum_{i=1}^{N} (y_i - \pmb{x}_i^\top\pmb{w})^2
\end{align}\]</div>
<p>重回帰モデルの学習において平均二乗残差を最小化するときは、学習データ<span class="math notranslate nohighlight">\(\mathcal{D}\)</span>や学習事例数<span class="math notranslate nohighlight">\(N\)</span>は事前に与えられる定数であり、パラメータ<span class="math notranslate nohighlight">\(\pmb{w}\)</span>は変数である。したがって、<span class="math notranslate nohighlight">\(N\)</span>は最小化の解に影響を与えないので（二乗残差の平均を最小化することと二乗残差の和を最小化することは同じ）、今後は目的関数に含めないことにする。すると、重回帰で最小化したい目的関数は、</p>
<div class="math notranslate nohighlight" id="equation-eq-objective-function">
<span class="eqno">(2.17)<a class="headerlink" href="#equation-eq-objective-function" title="この数式へのパーマリンク">¶</a></span>\[
\begin{align}
\hat{L}_{\mathcal{D}}(\pmb{w}) = \sum_{i=1}^{N} (y_i - \pmb{x}_i^\top\pmb{w})^2
\end{align}
\]</div>
<p>なお、全事例に関する残差をベクトル、</p>
<div class="amsmath math notranslate nohighlight" id="equation-f32c7cdf-e107-4b6a-ad5c-df56fefbc36f">
<span class="eqno">(2.18)<a class="headerlink" href="#equation-f32c7cdf-e107-4b6a-ad5c-df56fefbc36f" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\pmb{\epsilon} = \begin{pmatrix}
\epsilon_1 \\ \epsilon_2 \\ \dots \\ \epsilon_N
\end{pmatrix}
\end{align}\]</div>
<p>で表すことにすると、</p>
<div class="math notranslate nohighlight" id="equation-eq-mra-epsilon">
<span class="eqno">(2.19)<a class="headerlink" href="#equation-eq-mra-epsilon" title="この数式へのパーマリンク">¶</a></span>\[
\begin{align}
\pmb{\epsilon} = \pmb{y} - \hat{\pmb{y}} = \pmb{y} - \pmb{X}\pmb{w}
\end{align}
\]</div>
<p>である。</p>
<div class="margin sidebar">
<p class="sidebar-title">ベクトルの<span class="math notranslate nohighlight">\(L_2\)</span>ノルム</p>
<p><span class="math notranslate nohighlight">\(d\)</span>次元ベクトル、</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\pmb{v} &amp;= \begin{pmatrix}v_1 &amp; v_2 &amp; \dots &amp; v_d\end{pmatrix}^\top
\end{align*}\]</div>
<p>があるとき、その<span class="math notranslate nohighlight">\(L_2\)</span>ノルム<span class="math notranslate nohighlight">\(\lVert \pmb{v} \rVert\)</span>は、</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\lVert \pmb{v} \rVert &amp;= \sqrt{\pmb{v}^\top \pmb{v}} \\
&amp;= \sqrt{v_1^2 + v_2^2 + \dots + v_d^2}
\end{align*}\]</div>
<p>と計算される。</p>
</div>
<p>ベクトルや行列による表現形式を採用すると、重回帰で最小化したい目的関数は、</p>
<div class="important admonition">
<p class="admonition-title">重回帰の目的関数（二乗残差和）</p>
<div class="amsmath math notranslate nohighlight" id="equation-7772cc50-dbda-43be-9a37-52eef26f076c">
<span class="eqno">(2.20)<a class="headerlink" href="#equation-7772cc50-dbda-43be-9a37-52eef26f076c" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\hat{L}_{\mathcal{D}}(\pmb{w}) = \lVert \pmb{\epsilon} \rVert^2 = \lVert \pmb{y} - \pmb{X}\pmb{w} \rVert^2 = \lVert \pmb{X}\pmb{w} - \pmb{y} \rVert^2
\end{align}\]</div>
</div>
<p>と表現できる。</p>
</div>
<div class="section" id="mra-minimization">
<span id="id6"></span><h2><span class="section-number">2.4. </span>目的関数の最小化<a class="headerlink" href="#mra-minimization" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>目的関数<span class="math notranslate nohighlight">\(\hat{L}_{\mathcal{D}}(\pmb{w})\)</span>を最小にするパラメータのベクトル<span class="math notranslate nohighlight">\(\pmb{w}\)</span>を求めるため、式<a class="reference internal" href="#equation-eq-objective-function">(2.17)</a>をあるパラメータ<span class="math notranslate nohighlight">\(w_j\)</span>で偏微分する（<span class="math notranslate nohighlight">\(j \in \{1, 2, \dots, d\}\)</span>はパラメータのインデックスである）。</p>
<div class="margin sidebar">
<p class="sidebar-title">行列の行ベクトル</p>
<p>行列<span class="math notranslate nohighlight">\(\pmb{X} \in \mathbb{R}^{N \times d}\)</span>に対して、その<span class="math notranslate nohighlight">\(j\)</span>行目の要素を横に並べたベクトルを<strong>行ベクトル</strong>と呼び、<span class="math notranslate nohighlight">\(\pmb{X}_j\)</span>で表す。<span class="math notranslate nohighlight">\(\pmb{X}_j \in \mathbb{R}^{1 \times d}\)</span>である。</p>
</div>
<div class="math notranslate nohighlight" id="equation-eq-mra-grad-wj">
<span class="eqno">(2.21)<a class="headerlink" href="#equation-eq-mra-grad-wj" title="この数式へのパーマリンク">¶</a></span>\[\begin{split}
\begin{align}
\frac{\partial \hat{L}_{\mathcal{D}}(\pmb{w})}{\partial w_j}
&amp;= \sum_{i=1}^N 2 \cdot \underbrace{(y_i - \pmb{x}_i^\top\pmb{w})}_{=\epsilon_i} \cdot \{-\underbrace{(\pmb{x}_i)_j}_{=X_{i,j}}\} \\
&amp;= -2 \sum_{i=1}^N  X_{i,j} \epsilon_i \\
&amp;= -2 \begin{pmatrix}
X_{1,j} &amp;
X_{2,j} &amp;
\dots &amp;
X_{N,j} 
\end{pmatrix} \begin{pmatrix}
\epsilon_1 \\
\epsilon_2 \\
\dots \\
\epsilon_N
\end{pmatrix} \\
&amp;= -2 (\pmb{X}^\top)_j \pmb{\epsilon} \\
&amp;= -2 (\pmb{X}^\top \pmb{\epsilon})_j
\end{align}
\end{split}\]</div>
<p>なお、最後から２番目の式変形は、計画行列<span class="math notranslate nohighlight">\(\pmb{X}\)</span>の転置<span class="math notranslate nohighlight">\(\pmb{X}^\top\)</span>、</p>
<div class="amsmath math notranslate nohighlight" id="equation-d874099b-73a1-4949-86ac-cb048101f862">
<span class="eqno">(2.22)<a class="headerlink" href="#equation-d874099b-73a1-4949-86ac-cb048101f862" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\pmb{X}^\top = \begin{pmatrix}
X_{1,1} &amp; X_{2,1} &amp; \dots &amp; X_{N,1} \\
X_{1,2} &amp; X_{2,2} &amp; \dots &amp; X_{N,2} \\
\dots &amp; \dots &amp; \dots &amp; \dots \\
X_{1,j} &amp; X_{2,j} &amp; \dots &amp; X_{N,j} \\
\dots &amp; \dots &amp; \dots &amp; \dots \\
X_{1,d} &amp; X_{2,d} &amp; \dots &amp; X_{N,d}
\end{pmatrix}
\end{align}\]</div>
<p>の<span class="math notranslate nohighlight">\(j\)</span>行目の行ベクトルが<span class="math notranslate nohighlight">\(\begin{pmatrix} X_{1,j} &amp; X_{2,j} &amp; \dots &amp; X_{N,j}\end{pmatrix}\)</span>であることを利用した。最終行の式変形では、以下の行列ベクトル積の<span class="math notranslate nohighlight">\(j\)</span>行目の計算結果に基づく。</p>
<div class="amsmath math notranslate nohighlight" id="equation-ed0b5eab-818f-475a-978f-3d48dceda2a1">
<span class="eqno">(2.23)<a class="headerlink" href="#equation-ed0b5eab-818f-475a-978f-3d48dceda2a1" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\pmb{X}^\top \pmb{\epsilon} = \begin{pmatrix}
(\pmb{X}^\top)_{1} \pmb{\epsilon} \\
(\pmb{X}^\top)_{2} \pmb{\epsilon} \\
\dots \\
(\pmb{X}^\top)_{j} \pmb{\epsilon} \\
\dots \\
(\pmb{X}^\top)_{d} \pmb{\epsilon} \\
\end{pmatrix}
\end{align}\]</div>
<div class="margin sidebar">
<p class="sidebar-title"><span class="math notranslate nohighlight">\(\nabla\)</span>（ナブラ）と勾配</p>
<p>ここでは、多変数実関数の勾配を表すために<span class="math notranslate nohighlight">\(\nabla\)</span>を用いる。例えば、<span class="math notranslate nohighlight">\(xyz\)</span>直交座標空間<span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span>から実数への関数<span class="math notranslate nohighlight">\(f: \mathbb{R}^3 \longmapsto \mathbb{R}\)</span>があるとき、その<span class="math notranslate nohighlight">\(x, y, z\)</span>に関する偏微分を<span class="math notranslate nohighlight">\(3\)</span>次元ベクトルとしてまとめたものを、<span class="math notranslate nohighlight">\(\nabla f(x, y, z)\)</span>と書く。</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla f(x, y, z) &amp;= \begin{pmatrix}
\frac{\partial}{\partial x} \\
\frac{\partial}{\partial y} \\
\frac{\partial}{\partial z} \\
\end{pmatrix} f(x, y, z) \\
&amp;= \begin{pmatrix}
\frac{\partial f(x, y, z)}{\partial x} \\
\frac{\partial f(x, y, z)}{\partial y} \\
\frac{\partial f(x, y, z)}{\partial z} \\
\end{pmatrix}
\end{align*}\]</div>
<p><span class="math notranslate nohighlight">\(\nabla f(x, y, z)\)</span>は<span class="math notranslate nohighlight">\(f\)</span>の<strong>勾配</strong>と呼ばれ、<span class="math notranslate nohighlight">\(f(x, y, z)\)</span>の増加が最大となる方向を表す。</p>
<p>なお、<span class="math notranslate nohighlight">\(xyz\)</span>直交座標空間<span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span>において、演算子<span class="math notranslate nohighlight">\(\nabla\)</span>の元々の定義は、</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla = \pmb{e}_x \frac{\partial}{\partial x} + \pmb{e}_y \frac{\partial}{\partial y} + \pmb{e}_z \frac{\partial}{\partial z}
\end{align*}\]</div>
<p>ここで、<span class="math notranslate nohighlight">\(\pmb{e}_x, \pmb{e}_y, \pmb{e}_z\)</span>はそれぞれ、<span class="math notranslate nohighlight">\(x, y, z\)</span>方向の単位ベクトルである。したがって、先ほどの説明の通り<span class="math notranslate nohighlight">\(\nabla f(x, y, z)\)</span>は<span class="math notranslate nohighlight">\(f(x, y, z)\)</span>を<span class="math notranslate nohighlight">\(x, y, z\)</span>に関して偏微分したベクトルとなる。</p>
</div>
<p>式<a class="reference internal" href="#equation-eq-mra-grad-wj">(2.21)</a>より、<span class="math notranslate nohighlight">\(\frac{\partial \hat{L}_{\mathcal{D}}(\pmb{w})}{\partial w_j}\)</span>は<span class="math notranslate nohighlight">\(-2 (\pmb{X}^\top \pmb{\epsilon})\)</span>の<span class="math notranslate nohighlight">\(j\)</span>行目の要素に対応する。全ての<span class="math notranslate nohighlight">\(j \in \{1, 2, \dots, d\}\)</span>について偏微分したものを<span class="math notranslate nohighlight">\(d\)</span>次元ベクトルとしてまとめ、<span class="math notranslate nohighlight">\(\nabla \hat{L}_{\mathcal{D}}(\pmb{w})\)</span>と書くことにすると、</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla \hat{L}_{\mathcal{D}}(\pmb{w}) = \begin{pmatrix}
\frac{\partial }{\partial w_1} \\
\frac{\partial }{\partial w_2} \\
\dots \\
\frac{\partial }{\partial w_d} \\
\end{pmatrix} \hat{L}_{\mathcal{D}}(\pmb{w})
= -2 \pmb{X}^\top \pmb{\epsilon}
\end{split}\]</div>
<p>さらに、式<a class="reference internal" href="#equation-eq-mra-epsilon">(2.19)</a>より<span class="math notranslate nohighlight">\(\pmb{\epsilon} = \pmb{y} - \pmb{X}\pmb{w}\)</span>であるから、</p>
<div class="math notranslate nohighlight" id="equation-eq-gradient">
<span class="eqno">(2.24)<a class="headerlink" href="#equation-eq-gradient" title="この数式へのパーマリンク">¶</a></span>\[
\nabla \hat{L}_{\mathcal{D}}(\pmb{w}) = -2 \pmb{X}^\top \pmb{\epsilon} = 2 \pmb{X}^\top (\pmb{X}\pmb{w} - \pmb{y})
\]</div>
<p>目的関数を最小化するパラメータベクトル<span class="math notranslate nohighlight">\(\pmb{w}\)</span>を求めるため、これを<span class="math notranslate nohighlight">\(\pmb{0}\)</span>とおき、<span class="math notranslate nohighlight">\(\pmb{w}\)</span>について解くと、</p>
<div class="amsmath math notranslate nohighlight" id="equation-3ccb467d-0838-4e35-b365-17a9b75bc813">
<span class="eqno">(2.25)<a class="headerlink" href="#equation-3ccb467d-0838-4e35-b365-17a9b75bc813" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
2\pmb{X}^\top (\pmb{X}\pmb{w} - \pmb{y}) &amp;= \pmb{0} \\
\pmb{X}^\top \pmb{X}\pmb{w} - \pmb{X}^\top \pmb{y} &amp;= \pmb{0} \\
\pmb{X}^\top \pmb{X}\pmb{w} &amp;= \pmb{X}^\top \pmb{y} \\
\pmb{w} &amp;= (\pmb{X}^\top \pmb{X})^{-1}\pmb{X}^\top \pmb{y}
\end{align}\]</div>
<p>したがって、学習データに対応する計画行列<span class="math notranslate nohighlight">\(\pmb{X}\)</span>と目的変数ベクトル<span class="math notranslate nohighlight">\(\pmb{y}\)</span>が与えられると、平均二乗残差を最小にするパラメータベクトル<span class="math notranslate nohighlight">\(\pmb{w}\)</span>は行列演算で解析的に（閉じた式で）求めることができる。なお、<span class="math notranslate nohighlight">\((\pmb{X}^\top \pmb{X})^{-1}\pmb{X}^\top\)</span>を求める部分は、<a class="reference external" href="https://ja.wikipedia.org/wiki/%E6%93%AC%E4%BC%BC%E9%80%86%E8%A1%8C%E5%88%97">ムーア・ペンローズの擬似逆行列（一般化逆行列）</a>でも十分であるが、ここでは詳しく説明しない。</p>
<p>まとめとして、<span class="math notranslate nohighlight">\(\pmb{w}\)</span>を求める式を再掲する。</p>
<div class="important admonition">
<p class="admonition-title">重回帰のパラメータを求める式</p>
<div class="math notranslate nohighlight" id="equation-eq-closed-form-solution">
<span class="eqno">(2.26)<a class="headerlink" href="#equation-eq-closed-form-solution" title="この数式へのパーマリンク">¶</a></span>\[
\begin{align}
\pmb{w} &amp;= (\pmb{X}^\top \pmb{X})^{-1}\pmb{X}^\top \pmb{y}
\end{align}
\]</div>
</div>
<p>なお、導出の途中で出てきた以下の等式は<strong>正規方程式</strong>（normal equation）と呼ばれる。</p>
<div class="important admonition">
<p class="admonition-title">正規方程式</p>
<div class="math notranslate nohighlight" id="equation-eq-normal-equation">
<span class="eqno">(2.27)<a class="headerlink" href="#equation-eq-normal-equation" title="この数式へのパーマリンク">¶</a></span>\[
\pmb{X}^\top \pmb{X}\pmb{w} = \pmb{X}^\top \pmb{y}
\]</div>
</div>
</div>
<div class="section" id="id7">
<h2><span class="section-number">2.5. </span>目的関数の微分（別の導出）<a class="headerlink" href="#id7" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>重回帰の目的関数をベクトルの内積として記述し、パラメータベクトル<span class="math notranslate nohighlight">\(\pmb{w}\)</span>で微分する導出も紹介しておく。</p>
<div class="margin sidebar">
<p class="sidebar-title">行列の転置の公式</p>
<p><span class="math notranslate nohighlight">\((A^\top)^\top = A\)</span></p>
<p><span class="math notranslate nohighlight">\((AB)^\top = B^\top A^\top\)</span></p>
<p><span class="math notranslate nohighlight">\((ABC)^\top = C^\top B^\top A^\top\)</span></p>
</div>
<div class="amsmath math notranslate nohighlight" id="equation-529b3a09-38fa-4731-b100-012a8fcf05ab">
<span class="eqno">(2.28)<a class="headerlink" href="#equation-529b3a09-38fa-4731-b100-012a8fcf05ab" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\hat{L}_{\mathcal{D}}(\pmb{w}) &amp;= \sum_{i=1}^{N} (y_i - \pmb{x}_i^\top\pmb{w})^2 \\
&amp;= (\pmb{y} - \pmb{X}\pmb{w})^\top(\pmb{y} - \pmb{X}\pmb{w}) \\
&amp;= (\pmb{y}^\top - \pmb{w}^\top \pmb{X}^\top)(\pmb{y} - \pmb{X}\pmb{w}) \\
&amp;= \pmb{y}^\top \pmb{y} - \pmb{y}^\top \pmb{X}\pmb{w} - \pmb{w}^\top \pmb{X}^\top \pmb{y} + \pmb{w}^\top \pmb{X}^\top 
\end{align}\]</div>
<p>ここで、<span class="math notranslate nohighlight">\(\pmb{y}^\top \pmb{X}\pmb{w} = (\pmb{w}^\top \pmb{X}^\top \pmb{y})^\top\)</span>であるから、上式の第2項は第3項の計算結果の転置を取ったものである。また、この計算結果の次元は<span class="math notranslate nohighlight">\(1 \times 1\)</span>（スカラー）であるから、<span class="math notranslate nohighlight">\(\pmb{y}^\top \pmb{X}\pmb{w} = (\pmb{w}^\top \pmb{X}^\top \pmb{y})^\top = \pmb{w}^\top \pmb{X}^\top \pmb{y}\)</span>。ゆえに、</p>
<div class="amsmath math notranslate nohighlight" id="equation-c02ddb48-10fc-428b-8e4a-37f6cdc9d9b5">
<span class="eqno">(2.29)<a class="headerlink" href="#equation-c02ddb48-10fc-428b-8e4a-37f6cdc9d9b5" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\hat{L}_{\mathcal{D}}(\pmb{w}) = \pmb{y}^\top \pmb{y} - 2\pmb{w}^\top \pmb{X}^\top \pmb{y} + \pmb{w}^\top \pmb{X}^\top \pmb{X}\pmb{w} \\
\end{align}\]</div>
<p>そして、目的関数<span class="math notranslate nohighlight">\(\hat{L}_{\mathcal{D}}(\pmb{w})\)</span>を最小化する<span class="math notranslate nohighlight">\(\pmb{w}\)</span>を求めるために、<span class="math notranslate nohighlight">\(\hat{L}_{\mathcal{D}}(\pmb{w})\)</span>を<span class="math notranslate nohighlight">\(\pmb{w}\)</span>で偏微分する。</p>
<div class="amsmath math notranslate nohighlight" id="equation-3a9d4d77-16c3-4c5c-a28b-f6690cb50200">
<span class="eqno">(2.30)<a class="headerlink" href="#equation-3a9d4d77-16c3-4c5c-a28b-f6690cb50200" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\nabla \hat{L}_{\mathcal{D}}(\pmb{w}) = 
\frac{\partial }{\partial \pmb{w}}(\hat{L}_{\mathcal{D}}(\pmb{w})) &amp;= \frac{\partial}{\partial \pmb{w}} \left(\pmb{y}^\top \pmb{y} - 2\pmb{w}^\top \pmb{X}^\top \pmb{y} + \pmb{w}^\top \pmb{X}^\top \pmb{X}\pmb{w}\right) \\
  &amp;= 0 - \frac{\partial}{\partial \pmb{w}}\left(2\pmb{w}^\top \pmb{X}^\top \pmb{y}\right) + \frac{\partial}{\partial \pmb{w}}\left(\pmb{w}^\top \pmb{X}^\top \pmb{X}\pmb{w}\right)
\end{align}\]</div>
<p>上式の第2項は、</p>
<div class="amsmath math notranslate nohighlight" id="equation-d8027384-a103-4be6-8268-0762d4cda587">
<span class="eqno">(2.31)<a class="headerlink" href="#equation-d8027384-a103-4be6-8268-0762d4cda587" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\frac{\partial}{\partial \pmb{w}}\left(2\pmb{w}^\top \pmb{X}^\top \pmb{y}\right)
= 2\pmb{X}^\top \pmb{y}
\end{align}\]</div>
<div class="margin sidebar">
<p class="sidebar-title">二次形式の微分の公式</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial}{\partial \pmb{x}} \pmb{x}A\pmb{x}^\top = (A + A^\top)\pmb{x}\)</span></p>
</div>
<p>上式の第3項は、<span class="math notranslate nohighlight">\(\pmb{X}^\top \pmb{X}\)</span>が正方行列であり、<span class="math notranslate nohighlight">\((\pmb{X}^\top \pmb{X})^\top=\pmb{X}^\top (\pmb{X}^\top)^\top = \pmb{X}^\top \pmb{X}\)</span>である（<span class="math notranslate nohighlight">\(\pmb{X}^\top \pmb{X}\)</span>は対称行列である）ことに注意すると、</p>
<div class="amsmath math notranslate nohighlight" id="equation-c455b15c-0810-4233-86b9-55303b56ff5c">
<span class="eqno">(2.32)<a class="headerlink" href="#equation-c455b15c-0810-4233-86b9-55303b56ff5c" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\frac{\partial}{\partial \pmb{w}}\left(\pmb{w}^\top \pmb{X}^\top \pmb{X}\pmb{w}\right)
= \frac{\partial}{\partial \pmb{w}}\left(\pmb{w}^\top (\pmb{X}^\top \pmb{X}) \pmb{w}\right)
= \left((\pmb{X}^\top \pmb{X}) + (\pmb{X}^\top \pmb{X})^\top \right) \pmb{w}
= 2\pmb{X}^\top \pmb{X}\pmb{w}
\end{align}\]</div>
<p>ゆえに、</p>
<div class="amsmath math notranslate nohighlight" id="equation-455bb996-205f-4a82-890f-5d532783b38a">
<span class="eqno">(2.33)<a class="headerlink" href="#equation-455bb996-205f-4a82-890f-5d532783b38a" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\nabla \hat{L}_{\mathcal{D}}(\pmb{w}) &amp;= - \frac{\partial}{\partial \pmb{w}}\left(2\pmb{w}^\top \pmb{X}^\top \pmb{y}\right) + \frac{\partial}{\partial \pmb{w}}\left(\pmb{w}^\top \pmb{X}^\top \pmb{X}\pmb{w}\right) \\
  &amp;= - 2\pmb{X}^\top \pmb{y} + 2\pmb{X}^\top \pmb{X}\pmb{w} \\
  &amp;= 2\pmb{X}^\top (\pmb{X}\pmb{w} - \pmb{y})
\end{align}\]</div>
<p>したがって、式<a class="reference internal" href="#equation-eq-gradient">(2.24)</a>で求めた勾配と一致する。</p>
</div>
<div class="section" id="id8">
<h2><span class="section-number">2.6. </span>目的関数は凸関数<a class="headerlink" href="#id8" title="このヘッドラインへのパーマリンク">¶</a></h2>
<div class="margin sidebar">
<p class="sidebar-title">ヘッセ行列</p>
<p>多変数実関数の二階微分を行列で表したものである。例えば、<span class="math notranslate nohighlight">\(xyz\)</span>直交座標空間<span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span>から実数<span class="math notranslate nohighlight">\(\mathbb{R}\)</span>への関数<span class="math notranslate nohighlight">\(f(x,y,z)\)</span>に対し、</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\begin{pmatrix}
\frac{\partial^2 f}{\partial^2 x} &amp; \frac{\partial^2 f}{\partial x \partial y} &amp; \frac{\partial^2 f}{\partial x \partial z} \\
\frac{\partial^2 f}{\partial y \partial x} &amp; \frac{\partial^2 f}{\partial^2 y} &amp; \frac{\partial^2 f}{\partial y \partial z} \\
\frac{\partial^2 f}{\partial z \partial x} &amp; \frac{\partial^2 f}{\partial z \partial y} &amp; \frac{\partial^2 f}{\partial^2 z} \\
\end{pmatrix}
\end{align*}\]</div>
<p>を<span class="math notranslate nohighlight">\(f\)</span>のヘッセ行列と呼ぶ。</p>
</div>
<p>ここで、重回帰の目的関数は凸関数であることを示しておく。証明の方針は、目的関数の二階微分（ヘッセ行列）が半正定値行列であることを示すことである。</p>
<p>式<a class="reference internal" href="#equation-eq-gradient">(2.24)</a>より、目的関数の一階微分は、</p>
<div class="amsmath math notranslate nohighlight" id="equation-d08cccce-c301-4485-978c-672787780e68">
<span class="eqno">(2.34)<a class="headerlink" href="#equation-d08cccce-c301-4485-978c-672787780e68" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\nabla \hat{L}_{\mathcal{D}}(\pmb{w})
= \begin{pmatrix}
\frac{\partial}{\partial w_1} \\
\frac{\partial}{\partial w_2} \\
\dots \\
\frac{\partial}{\partial w_d} \\
\end{pmatrix} \hat{L}_{\mathcal{D}}(\pmb{w})
= 2 \pmb{X}^\top \pmb{X}\pmb{w} - \pmb{y}
\end{align}\]</div>
<div class="margin sidebar">
<p class="sidebar-title"><span class="math notranslate nohighlight">\(\nabla^2\)</span>と二階微分</p>
<p><span class="math notranslate nohighlight">\(\nabla^2\)</span>はラプラシアンかヘッセ行列を表す。<span class="math notranslate nohighlight">\(xyz\)</span>直交座標空間<span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span>において、<span class="math notranslate nohighlight">\(\nabla^2\)</span>を内積<span class="math notranslate nohighlight">\(\nabla \cdot \nabla\)</span>とすると、ラプラシアンとなる。</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla \cdot \nabla &amp;= \begin{pmatrix}
\frac{\partial}{\partial x} \\
\frac{\partial}{\partial y} \\
\frac{\partial}{\partial z} \\
\end{pmatrix} \cdot \begin{pmatrix}
\frac{\partial}{\partial x} \\
\frac{\partial}{\partial y} \\
\frac{\partial}{\partial z} \\
\end{pmatrix} \\
&amp;=
\frac{\partial^2}{\partial^2 x} +
\frac{\partial^2}{\partial^2 y} +
\frac{\partial^2}{\partial^2 z}
\end{align*}\]</div>
<p>これに対し、<span class="math notranslate nohighlight">\(\nabla^2\)</span>を直積<span class="math notranslate nohighlight">\(\nabla \nabla^\top\)</span>と考えると、ヘッセ行列となる。</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla \nabla^\top &amp;= \begin{pmatrix}
\frac{\partial}{\partial x} \\
\frac{\partial}{\partial y} \\
\frac{\partial}{\partial z} \\
\end{pmatrix} \begin{pmatrix}
\frac{\partial}{\partial x} &amp;
\frac{\partial}{\partial y} &amp;
\frac{\partial}{\partial z}
\end{pmatrix} \\
&amp;= \begin{pmatrix}
\frac{\partial^2}{\partial^2 x} &amp; \frac{\partial^2}{\partial x \partial y} &amp; \frac{\partial^2}{\partial x \partial z} \\
\frac{\partial^2}{\partial y \partial x} &amp; \frac{\partial^2}{\partial^2 y} &amp; \frac{\partial^2}{\partial y \partial z} \\
\frac{\partial^2}{\partial z \partial x} &amp; \frac{\partial^2}{\partial z \partial y} &amp; \frac{\partial^2}{\partial^2 z} \\
\end{pmatrix}
\end{align*}\]</div>
<p>本資料では、<span class="math notranslate nohighlight">\(\nabla^2\)</span>でヘッセ行列を表すこととする。</p>
</div>
<p>さらに、目的関数の二階微分（ヘッセ行列）は、</p>
<div class="amsmath math notranslate nohighlight" id="equation-7fa31f84-1b72-48c9-9ac3-e22a2ddba14b">
<span class="eqno">(2.35)<a class="headerlink" href="#equation-7fa31f84-1b72-48c9-9ac3-e22a2ddba14b" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\nabla^2 \hat{L}_{\mathcal{D}}(\pmb{w})
&amp;= \begin{pmatrix}
\frac{\partial^2}{\partial^2 w_1} &amp; \frac{\partial^2}{\partial w_1 \partial w_2} &amp; \dots &amp; \frac{\partial^2}{\partial w_1 \partial w_d} \\
\frac{\partial^2}{\partial w_2 \partial w_1} &amp; \frac{\partial^2}{\partial^2 w_2} &amp; \dots &amp; \frac{\partial^2}{\partial w_2 \partial w_d} \\
\dots \\
\frac{\partial^2}{\partial w_d \partial w_1} &amp; \frac{\partial^2}{\partial w_d \partial w_2} &amp; \dots &amp; \frac{\partial^2}{\partial^2 w_d}
\end{pmatrix} \hat{L}_{\mathcal{D}}(\pmb{w}) \\
&amp;= \frac{\partial}{\partial \pmb{w}} (2 \pmb{X}^\top (\pmb{X}\pmb{w} - \pmb{y})) \\
&amp;= \frac{\partial}{\partial \pmb{w}} (2 \pmb{X}^\top \pmb{X}\pmb{w} - 2 \pmb{X}^\top \pmb{y}) \\
&amp;= 2 \pmb{X}^\top \pmb{X}
\end{align}\]</div>
<p>ここで、任意の非ゼロベクトル<span class="math notranslate nohighlight">\(\pmb{u}\)</span>に対して、</p>
<div class="amsmath math notranslate nohighlight" id="equation-0402a563-ad66-43ad-9e8a-047bcbc70013">
<span class="eqno">(2.36)<a class="headerlink" href="#equation-0402a563-ad66-43ad-9e8a-047bcbc70013" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\pmb{u}^\top (2 \pmb{X}^\top \pmb{X}) \pmb{u} = 2 \pmb{u}^\top \pmb{X}^\top \pmb{X} \pmb{u} = 2(\pmb{X}\pmb{u})^\top (\pmb{X} \pmb{u}) = 2 \|\pmb{X} \pmb{u}\|^2 \geq 0
\end{align}\]</div>
<p>であるから、目的関数の二階微分<span class="math notranslate nohighlight">\(2 \pmb{X}^\top \pmb{X}\)</span>は半正定値行列である。ゆえに、重回帰の目的関数は凸関数である。</p>
<div class="margin sidebar">
<p class="sidebar-title">二階微分が半正定値行列ならば凸関数</p>
<p>ある多変数実関数<span class="math notranslate nohighlight">\(f\)</span>の二階微分が半正定値行列ならば、<span class="math notranslate nohighlight">\(f\)</span>は凸関数である。本資料では、その証明を省略するので、凸最適化の教科書などを参照されたい。</p>
<p>１変数実関数<span class="math notranslate nohighlight">\(f(x)\)</span>に関して、<span class="math notranslate nohighlight">\(f''(x) \geq 0\)</span>ならば勾配<span class="math notranslate nohighlight">\(f'(x)\)</span>は単調増加であるため、<span class="math notranslate nohighlight">\(f(x)\)</span>は凸関数である。それを多変数関数に拡張したものが、半正定値性による凸関数の判定とイメージするとよい。</p>
</div>
</div>
<div class="section" id="id9">
<h2><span class="section-number">2.7. </span>重回帰の性質<a class="headerlink" href="#id9" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>重回帰に関する様々な性質を導出してみよう <span id="id10">[<a class="reference internal" href="../reference.html#id5">Beck, 2009</a>]</span>。重回帰で推定されたパラメータ<span class="math notranslate nohighlight">\(\pmb{w}\)</span>は、式<a class="reference internal" href="#equation-eq-gradient">(2.24)</a>を<span class="math notranslate nohighlight">\(\pmb{0}\)</span>とおいたものであるので、</p>
<div class="math notranslate nohighlight" id="equation-eq-x-top-epsilon">
<span class="eqno">(2.37)<a class="headerlink" href="#equation-eq-x-top-epsilon" title="この数式へのパーマリンク">¶</a></span>\[\begin{split}
\begin{align}
\nabla \hat{L}_{\mathcal{D}}(\pmb{w}) = -2 \pmb{X}^\top \pmb{\epsilon} &amp;= \pmb{0} \\
\pmb{X}^\top \pmb{\epsilon} &amp;= \pmb{0}
\end{align}
\end{split}\]</div>
<p>この行列・ベクトルの成分を明示すると、</p>
<div class="amsmath math notranslate nohighlight" id="equation-864a888e-b388-44dd-813e-d3796a554906">
<span class="eqno">(2.38)<a class="headerlink" href="#equation-864a888e-b388-44dd-813e-d3796a554906" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\begin{pmatrix}
X_{1,1} &amp; X_{2,1} &amp; \dots &amp; X_{N,1} \\
X_{1,2} &amp; X_{2,2} &amp; \dots &amp; X_{N,2} \\
\dots &amp; \dots &amp; \dots &amp; \dots \\
X_{1,d} &amp; X_{2,d} &amp; \dots &amp; X_{N,d}
\end{pmatrix}
\begin{pmatrix}
\epsilon_1 \\
\epsilon_2 \\
\dots \\
\epsilon_N
\end{pmatrix}
=
\begin{pmatrix}
\sum_{i=1}^N X_{i,1} \epsilon_i \\
\sum_{i=1}^N X_{i,2} \epsilon_i \\
\dots \\
\sum_{i=1}^N X_{i,d} \epsilon_i
\end{pmatrix}
=
\begin{pmatrix}
0 \\ 0 \\ \dots \\ 0
\end{pmatrix}
\end{align}\]</div>
<p>ゆえに、任意の整数<span class="math notranslate nohighlight">\(j \in \{1, 2, \dots, d\}\)</span>に関して、</p>
<div class="math notranslate nohighlight" id="equation-eq-epsilon-property">
<span class="eqno">(2.39)<a class="headerlink" href="#equation-eq-epsilon-property" title="この数式へのパーマリンク">¶</a></span>\[
\sum_{i=1}^N X_{i,j} \epsilon_i = 0
\]</div>
<p>これは、<span class="math notranslate nohighlight">\(\pmb{w}\)</span>を求めるときに、式<a class="reference internal" href="#equation-eq-mra-grad-wj">(2.21)</a>の2行目を全ての<span class="math notranslate nohighlight">\(j\)</span>に対して<span class="math notranslate nohighlight">\(0\)</span>としたことからも、明らかである。</p>
<p>ここで、バイアス項（切片）に対応するパラメータを持たせるため、<span class="math notranslate nohighlight">\(\pmb{X}\)</span>の先頭列の要素はすべて<span class="math notranslate nohighlight">\(1\)</span>と仮定する。</p>
<div class="math notranslate nohighlight" id="equation-eq-first-column-one">
<span class="eqno">(2.40)<a class="headerlink" href="#equation-eq-first-column-one" title="この数式へのパーマリンク">¶</a></span>\[
\begin{array}{cc}
X_{i,1} = 1 &amp; (i \in \{1, 2, \dots, N\})
\end{array}
\]</div>
<p>念のため、<span class="math notranslate nohighlight">\(\pmb{X}\)</span>と<span class="math notranslate nohighlight">\(\pmb{X}^\top\)</span>を明示的に書くと、</p>
<div class="amsmath math notranslate nohighlight" id="equation-98aae7c4-b656-4a98-84af-ecc0f28c8e60">
<span class="eqno">(2.41)<a class="headerlink" href="#equation-98aae7c4-b656-4a98-84af-ecc0f28c8e60" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\pmb{X} = 
\begin{pmatrix}
1 &amp; X_{1,2} &amp; \dots &amp; X_{1,d} \\
1 &amp; X_{2,2} &amp; \dots &amp; X_{2,d} \\
\dots &amp; \dots &amp; \dots &amp; \dots \\
1 &amp; X_{1,N} &amp; \dots &amp; X_{N,d}
\end{pmatrix}, 
\pmb{X}^\top = 
\begin{pmatrix}
1 &amp; 1 &amp; \dots &amp; 1 \\
X_{1,2} &amp; X_{2,2} &amp; \dots &amp; X_{N,2} \\
\dots &amp; \dots &amp; \dots &amp; \dots \\
X_{1,d} &amp; X_{2,d} &amp; \dots &amp; X_{N,d}
\end{pmatrix}
\end{align}\]</div>
<p>これにより、<span class="math notranslate nohighlight">\(w_1\)</span>がバイアス項に対応するパラメータとなる。ここから、重回帰の様々な性質を導出してみよう。なお、バイアス項に対応させる列は先頭である必要はなく、その他の場所でも以降の議論の一般性は失われない。</p>
<div class="section" id="id11">
<h3><span class="section-number">2.7.1. </span>残差の和および平均は<span class="math notranslate nohighlight">\(0\)</span><a class="headerlink" href="#id11" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p><span class="math notranslate nohighlight">\(j=1\)</span>として式<a class="reference internal" href="#equation-eq-epsilon-property">(2.39)</a>に式<a class="reference internal" href="#equation-eq-first-column-one">(2.40)</a>を代入すると、</p>
<div class="math notranslate nohighlight" id="equation-eq-sum-epsilon-zero-mra">
<span class="eqno">(2.42)<a class="headerlink" href="#equation-eq-sum-epsilon-zero-mra" title="この数式へのパーマリンク">¶</a></span>\[
\sum_{i=1}^N X_{i,1} \epsilon_i = \sum_{i=1}^N \epsilon_i = 0
\]</div>
<p>ゆえに、残差の和および平均は<span class="math notranslate nohighlight">\(0\)</span>である。</p>
</div>
<div class="section" id="hat-y-y">
<h3><span class="section-number">2.7.2. </span>目的変数の推定値<span class="math notranslate nohighlight">\(\hat{y}\)</span>の平均値は観測値<span class="math notranslate nohighlight">\(y\)</span>の平均に等しい<a class="headerlink" href="#hat-y-y" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>残差の定義<span class="math notranslate nohighlight">\(\pmb{\epsilon} = \pmb{y} - \hat{\pmb{y}}\)</span>を明示的に書くと、</p>
<div class="amsmath math notranslate nohighlight" id="equation-03e75cc9-0594-4f23-af5d-41af740a6b85">
<span class="eqno">(2.43)<a class="headerlink" href="#equation-03e75cc9-0594-4f23-af5d-41af740a6b85" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\begin{pmatrix}
\epsilon_1 \\ \epsilon_2 \\ \dots \\ \epsilon_N
\end{pmatrix} =
\begin{pmatrix}
y_1 \\ y_2 \\ \dots \\ y_N
\end{pmatrix}
-
\begin{pmatrix}
\hat{y}_1 \\ \hat{y}_2 \\ \dots \\ \hat{y}_N
\end{pmatrix}
\end{align}\]</div>
<p>両辺の各要素の和をとると、</p>
<div class="amsmath math notranslate nohighlight" id="equation-10da1559-a765-49ec-8fec-7886b29dcfa0">
<span class="eqno">(2.44)<a class="headerlink" href="#equation-10da1559-a765-49ec-8fec-7886b29dcfa0" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\sum_{i=1}^N \epsilon_i = \sum_{i=1}^N y_i - \sum_{i=1}^N \hat{y}_i
\end{align}\]</div>
<p>ここで、式<a class="reference internal" href="#equation-eq-sum-epsilon-zero-mra">(2.42)</a>より左辺は<span class="math notranslate nohighlight">\(0\)</span>であるから、</p>
<div class="amsmath math notranslate nohighlight" id="equation-1a20730c-0d4d-4f51-b1fc-cf5f54abff9e">
<span class="eqno">(2.45)<a class="headerlink" href="#equation-1a20730c-0d4d-4f51-b1fc-cf5f54abff9e" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\sum_{i=1}^N y_i &amp;= \sum_{i=1}^N \hat{y}_i \\
\frac{1}{N}\sum_{i=1}^N y_i &amp;= \frac{1}{N}\sum_{i=1}^N \hat{y}_i \\
\overline{y} &amp;= \overline{\hat{y}}
\end{align}\]</div>
<p>ゆえに、目的変数の推定値<span class="math notranslate nohighlight">\(\hat{y}\)</span>の平均値は観測値<span class="math notranslate nohighlight">\(y\)</span>の平均に等しい。</p>
</div>
<div class="section" id="id12">
<h3><span class="section-number">2.7.3. </span>観測データの重心は回帰平面上に存在する<a class="headerlink" href="#id12" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>説明変数の平均ベクトル<span class="math notranslate nohighlight">\(\overline{\pmb{x}}\)</span>、</p>
<div class="amsmath math notranslate nohighlight" id="equation-24818d59-4224-4957-a356-895915df918c">
<span class="eqno">(2.46)<a class="headerlink" href="#equation-24818d59-4224-4957-a356-895915df918c" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\overline{\pmb{x}} = \frac{1}{N} \sum_{i=1}^N \pmb{x}_i
\end{align}\]</div>
<p>に対して、目的変数の推定値を計算すると、</p>
<div class="amsmath math notranslate nohighlight" id="equation-3991e92f-e0ed-4f13-aa59-fec624c6bfbf">
<span class="eqno">(2.47)<a class="headerlink" href="#equation-3991e92f-e0ed-4f13-aa59-fec624c6bfbf" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\overline{\pmb{x}}^\top \pmb{w}
&amp;= \frac{1}{N} \sum_{i=1}^N \pmb{x}_i^\top \pmb{w} \\
&amp;= \frac{1}{N} \sum_{i=1}^N \hat{y}_i \\
&amp;= \overline{\hat{y}} = \overline{y}
\end{align}\]</div>
<p>目的変数の平均<span class="math notranslate nohighlight">\(\overline{y}\)</span>が得られる。ゆえに、観測データの重心は回帰平面上に存在する。</p>
</div>
<div class="section" id="id13">
<h3><span class="section-number">2.7.4. </span>各説明変数と残差には相関がない<a class="headerlink" href="#id13" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p><span class="math notranslate nohighlight">\(j\)</span>番目の説明変数<span class="math notranslate nohighlight">\(\pmb{X}_{:,j}\)</span>を確率変数と見なし、<span class="math notranslate nohighlight">\(\mathcal{X}_j\)</span>と書くことにする。
任意の説明変数<span class="math notranslate nohighlight">\(\mathcal{X}_j\)</span>と残差<span class="math notranslate nohighlight">\(\mathcal{E}\)</span>の共分散を計算する。残差の平均は<span class="math notranslate nohighlight">\(\overline{\epsilon}=0\)</span>であること、式<a class="reference internal" href="#equation-eq-epsilon-property">(2.39)</a>と式<a class="reference internal" href="#equation-eq-sum-epsilon-zero-mra">(2.42)</a>を利用すると、</p>
<div class="amsmath math notranslate nohighlight" id="equation-b2889ffe-0e39-4928-adad-bc481d0c9689">
<span class="eqno">(2.48)<a class="headerlink" href="#equation-b2889ffe-0e39-4928-adad-bc481d0c9689" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\mathrm{Cov}[\mathcal{X}_j, \mathcal{E}] &amp;= \frac{1}{N}\sum_{i=1}^N (X_{i,j} - \overline{\pmb{X}_{:,j}}) (\epsilon_i - \overline{\epsilon}) \\
&amp;= \frac{1}{N}\sum_{i=1}^N (X_{i,j} - \overline{\pmb{X}_{:,j}}) \epsilon_i \\
&amp;= \frac{1}{N}\sum_{i=1}^N X_{i,j}  \epsilon_i - \frac{1}{N}\sum_{i=1}^N  \overline{\pmb{X}_{:,j}} \epsilon_i \\
&amp;= \frac{1}{N}\underbrace{\sum_{i=1}^N X_{i,j} \epsilon_i}_{0} - \frac{\overline{\pmb{X}_{:,j}}}{N}\underbrace{\sum_{i=1}^N  \epsilon_i}_{0} \\
&amp;= 0
\end{align}\]</div>
</div>
<div class="section" id="id14">
<h3><span class="section-number">2.7.5. </span>目的変数の推定値と残差には相関がない<a class="headerlink" href="#id14" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>目的変数の推定値<span class="math notranslate nohighlight">\(\hat{\mathcal{Y}}\)</span>と残差<span class="math notranslate nohighlight">\(\mathcal{E}\)</span>の共分散を計算する。残差の平均は<span class="math notranslate nohighlight">\(\overline{\epsilon}=0\)</span>であること、式<a class="reference internal" href="#equation-eq-x-top-epsilon">(2.37)</a>と式<a class="reference internal" href="#equation-eq-sum-epsilon-zero-mra">(2.42)</a>を利用すると、</p>
<div class="math notranslate nohighlight" id="equation-eq-cov-hat-y-and-epsilon-mra">
<span class="eqno">(2.49)<a class="headerlink" href="#equation-eq-cov-hat-y-and-epsilon-mra" title="この数式へのパーマリンク">¶</a></span>\[\begin{split}
\begin{align}
\mathrm{Cov}[\hat{\mathcal{Y}},\mathcal{E}] &amp;= \frac{1}{N}\sum_{i=1}^N (\hat{y}_i - \overline{\hat{y}}) (\epsilon_i - \overline{\epsilon}) \\
&amp;= \frac{1}{N}\sum_{i=1}^N (\hat{y}_i - \overline{\hat{y}}) \epsilon_i \\
&amp;= \frac{1}{N}\sum_{i=1}^N \hat{y}_i \epsilon_i - \frac{1}{N}\sum_{i=1}^N \overline{\hat{y}} \epsilon_i \\
&amp;= \frac{1}{N}\sum_{i=1}^N \hat{y}_i \epsilon_i - \frac{\overline{\hat{y}}}{N}\underbrace{\sum_{i=1}^N \epsilon_i}_{0} \\
&amp;= \frac{1}{N}\sum_{i=1}^N \hat{y}_i \epsilon_i \\
&amp;= \frac{1}{N}\pmb{\hat{y}}^\top \pmb{\epsilon} \\
&amp;= \frac{1}{N}(\pmb{X} \pmb{w})^\top \pmb{\epsilon} \\
&amp;= \frac{1}{N}\pmb{w}^\top \underbrace{\pmb{X}^\top \pmb{\epsilon}}_{0} \\
&amp;= 0
\end{align}
\end{split}\]</div>
<p>であるから、<span class="math notranslate nohighlight">\(\mathrm{Cov}[\hat{\mathcal{Y}},\mathcal{E}] = 0\)</span>が言える。すなわち、目的変数の推定値と残差の共分散が0になることから、目的変数の推定値と残差は無相関（無関係）であることが分かる。</p>
</div>
</div>
<div class="section" id="id15">
<h2><span class="section-number">2.8. </span>決定係数<a class="headerlink" href="#id15" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>目的変数の観測値の分散<span class="math notranslate nohighlight">\(\mathrm{Var}[\mathcal{Y}]\)</span>は目的変数の推定値の分散<span class="math notranslate nohighlight">\(\mathrm{Var}[\hat{\mathcal{Y}}]\)</span>と残差の分散<span class="math notranslate nohighlight">\(\mathrm{Var}[\mathcal{E}]\)</span>の和で表されることを示す。</p>
<div class="amsmath math notranslate nohighlight" id="equation-fc509b60-609d-407c-8b36-31721ece7a16">
<span class="eqno">(2.50)<a class="headerlink" href="#equation-fc509b60-609d-407c-8b36-31721ece7a16" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
\mathrm{Var}[\mathcal{Y}] &amp;= \frac{1}{N} \sum_{i=1}^N (y_i - \overline{y})^2 \\
&amp;= \frac{1}{N} \sum_{i=1}^N \{(\hat{y}_i + \epsilon_i) - \overline{y}\}^2 \\
&amp;= \frac{1}{N} \sum_{i=1}^N \{(\hat{y}_i  - \overline{y}) + \epsilon_i\}^2 \\
&amp;= \frac{1}{N} \sum_{i=1}^N (\hat{y}_i  - \overline{y})^2 + \frac{1}{N} \sum_{i=1}^N 2 (\hat{y}_i  - \overline{y}) \epsilon_i + \frac{1}{N} \sum_{i=1}^N \epsilon_i^2 \\
&amp;= \mathrm{Var}[\hat{\mathcal{Y}}] + 2 \cdot \frac{1}{N} \underbrace{\sum_{i=1}^N \hat{y}_i \epsilon_i}_{0} - 2\overline{y} \cdot \frac{1}{N} \underbrace{\sum_{i=1}^N \epsilon_i}_{0} + \mathrm{Var}[\mathcal{E}] \\
&amp;= \mathrm{Var}[\hat{\mathcal{Y}}] + \mathrm{Var}[\mathcal{E}]
\end{align}\]</div>
<p>なお、以上の式変形における最後から2行目において、第２項は式<a class="reference internal" href="#equation-eq-cov-hat-y-and-epsilon-mra">(2.49)</a>の導出結果、第３項は式<a class="reference internal" href="#equation-eq-sum-epsilon-zero-mra">(2.42)</a>より<span class="math notranslate nohighlight">\(0\)</span>であることを利用した。</p>
<p>したがって、単回帰のときと同様に、</p>
<ul class="simple">
<li><p><strong>全変動</strong>: 目的変数の観測値の分散<span class="math notranslate nohighlight">\(\mathrm{Var}[\mathcal{Y}]\)</span>（データの変動）</p></li>
<li><p><strong>回帰変動</strong>: 目的変数の推定値の分散<span class="math notranslate nohighlight">\(\mathrm{Var}[\hat{\mathcal{Y}}]\)</span>（モデルの変動）</p></li>
<li><p><strong>残差変動</strong>: 残差の分散<span class="math notranslate nohighlight">\(\mathrm{Var}[\mathcal{E}]\)</span>（モデルが説明しきれなかった変動）</p></li>
</ul>
<p>すると、先ほどの式は「<strong>全変動は回帰変動と残差変動の和で表される</strong>」という関係を表している。</p>
<p>データの全変動のうち、回帰によって表現できた回帰変動の割合を<strong>決定係数</strong>とよび、次式で定義する。</p>
<div class="important admonition">
<p class="admonition-title">決定係数</p>
<div class="amsmath math notranslate nohighlight" id="equation-04abb1dc-13fa-44d7-a131-033aabf1ca32">
<span class="eqno">(2.51)<a class="headerlink" href="#equation-04abb1dc-13fa-44d7-a131-033aabf1ca32" title="この数式へのパーマリンク">¶</a></span>\[\begin{align}
R^2 = \frac{\mathrm{Var}[\hat{\mathcal{Y}}]}{\mathrm{Var}[\mathcal{Y}]} = \frac{\mathrm{Var}[\mathcal{Y}] - \mathrm{Var}[\mathcal{E}]}{\mathrm{Var}[\mathcal{Y}]} = 1 - \frac{\mathrm{Var}[\mathcal{E}]}{\mathrm{Var}[\mathcal{Y}]}
\end{align}\]</div>
</div>
<p>決定係数は<span class="math notranslate nohighlight">\([0,1]\)</span>の範囲の値ととり、データがモデル（単回帰の場合は直線）によく当てはまるときは<span class="math notranslate nohighlight">\(1\)</span>に近くなる。</p>
</div>
<div class="section" id="id16">
<h2><span class="section-number">2.9. </span>重回帰の実施例<a class="headerlink" href="#id16" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>冒頭の最高気温とアイスクリーム・シャーベットの支出額のデータに対して、単回帰を行う例を示す。まず、説明変数（最高気温）を<code class="docutils literal notranslate"><span class="pre">X</span></code>、目的変数（支出額）を<code class="docutils literal notranslate"><span class="pre">Y</span></code>に格納する。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_graph</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;最高気温とアイスクリーム・シャーベットの支出額&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;最高気温の月平均（℃）&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;支出額（円）&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">35</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">250</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
     <span class="mf">9.1</span><span class="p">,</span> <span class="mf">11.2</span><span class="p">,</span> <span class="mf">12.3</span><span class="p">,</span> <span class="mf">18.9</span><span class="p">,</span> <span class="mf">22.2</span><span class="p">,</span> <span class="mf">26.</span> <span class="p">,</span> <span class="mf">30.9</span><span class="p">,</span> <span class="mf">31.2</span><span class="p">,</span> <span class="mf">28.8</span><span class="p">,</span> <span class="mf">23.</span> <span class="p">,</span> <span class="mf">18.3</span><span class="p">,</span>
    <span class="mf">11.1</span><span class="p">,</span>  <span class="mf">8.3</span><span class="p">,</span>  <span class="mf">9.1</span><span class="p">,</span> <span class="mf">12.5</span><span class="p">,</span> <span class="mf">18.5</span><span class="p">,</span> <span class="mf">23.6</span><span class="p">,</span> <span class="mf">24.8</span><span class="p">,</span> <span class="mf">30.1</span><span class="p">,</span> <span class="mf">33.1</span><span class="p">,</span> <span class="mf">29.8</span><span class="p">,</span> <span class="mf">23.</span> <span class="p">,</span>
    <span class="mf">16.3</span><span class="p">,</span> <span class="mf">11.2</span><span class="p">,</span>  <span class="mf">9.6</span><span class="p">,</span> <span class="mf">10.3</span><span class="p">,</span> <span class="mf">16.4</span><span class="p">,</span> <span class="mf">19.2</span><span class="p">,</span> <span class="mf">24.1</span><span class="p">,</span> <span class="mf">26.5</span><span class="p">,</span> <span class="mf">31.4</span><span class="p">,</span> <span class="mf">33.2</span><span class="p">,</span> <span class="mf">28.8</span><span class="p">,</span>
    <span class="mf">23.</span> <span class="p">,</span> <span class="mf">17.4</span><span class="p">,</span> <span class="mf">12.1</span><span class="p">,</span> <span class="mf">10.6</span><span class="p">,</span>  <span class="mf">9.8</span><span class="p">,</span> <span class="mf">14.5</span><span class="p">,</span> <span class="mf">19.6</span><span class="p">,</span> <span class="mf">24.7</span><span class="p">,</span> <span class="mf">26.9</span><span class="p">,</span> <span class="mf">30.5</span><span class="p">,</span> <span class="mf">31.2</span><span class="p">,</span>
    <span class="mf">26.9</span><span class="p">,</span> <span class="mf">23.</span> <span class="p">,</span> <span class="mf">17.4</span><span class="p">,</span> <span class="mf">11.</span> <span class="p">,</span> <span class="mf">10.4</span><span class="p">,</span> <span class="mf">10.4</span><span class="p">,</span> <span class="mf">15.5</span><span class="p">,</span> <span class="mf">19.3</span><span class="p">,</span> <span class="mf">26.4</span><span class="p">,</span> <span class="mf">26.4</span><span class="p">,</span> <span class="mf">30.1</span><span class="p">,</span>
    <span class="mf">30.5</span><span class="p">,</span> <span class="mf">26.4</span><span class="p">,</span> <span class="mf">22.7</span><span class="p">,</span> <span class="mf">17.8</span><span class="p">,</span> <span class="mf">13.4</span><span class="p">,</span> <span class="mf">10.6</span><span class="p">,</span> <span class="mf">12.2</span><span class="p">,</span> <span class="mf">14.9</span><span class="p">,</span> <span class="mf">20.3</span><span class="p">,</span> <span class="mf">25.2</span><span class="p">,</span> <span class="mf">26.3</span><span class="p">,</span>
    <span class="mf">29.7</span><span class="p">,</span> <span class="mf">31.6</span><span class="p">,</span> <span class="mf">27.7</span><span class="p">,</span> <span class="mf">22.6</span><span class="p">,</span> <span class="mf">15.5</span><span class="p">,</span> <span class="mf">13.8</span><span class="p">,</span> <span class="mf">10.8</span><span class="p">,</span> <span class="mf">12.1</span><span class="p">,</span> <span class="mf">13.4</span><span class="p">,</span> <span class="mf">19.9</span><span class="p">,</span> <span class="mf">25.1</span><span class="p">,</span>
    <span class="mf">26.4</span><span class="p">,</span> <span class="mf">31.8</span><span class="p">,</span> <span class="mf">30.4</span><span class="p">,</span> <span class="mf">26.8</span><span class="p">,</span> <span class="mf">20.1</span><span class="p">,</span> <span class="mf">16.6</span><span class="p">,</span> <span class="mf">11.1</span><span class="p">,</span>  <span class="mf">9.4</span><span class="p">,</span> <span class="mf">10.1</span><span class="p">,</span> <span class="mf">16.9</span><span class="p">,</span> <span class="mf">22.1</span><span class="p">,</span>
    <span class="mf">24.6</span><span class="p">,</span> <span class="mf">26.6</span><span class="p">,</span> <span class="mf">32.7</span><span class="p">,</span> <span class="mf">32.5</span><span class="p">,</span> <span class="mf">26.6</span><span class="p">,</span> <span class="mf">23.</span> <span class="p">,</span> <span class="mf">17.7</span><span class="p">,</span> <span class="mf">12.1</span><span class="p">,</span> <span class="mf">10.3</span><span class="p">,</span> <span class="mf">11.6</span><span class="p">,</span> <span class="mf">15.4</span><span class="p">,</span>
    <span class="mf">19.</span> <span class="p">,</span> <span class="mf">25.3</span><span class="p">,</span> <span class="mf">25.8</span><span class="p">,</span> <span class="mf">27.5</span><span class="p">,</span> <span class="mf">32.8</span><span class="p">,</span> <span class="mf">29.4</span><span class="p">,</span> <span class="mf">23.3</span><span class="p">,</span> <span class="mf">17.7</span><span class="p">,</span> <span class="mf">12.6</span><span class="p">,</span> <span class="mf">11.1</span><span class="p">,</span> <span class="mf">13.3</span><span class="p">,</span>
    <span class="mf">16.</span> <span class="p">,</span> <span class="mf">18.2</span><span class="p">,</span> <span class="mf">24.</span> <span class="p">,</span> <span class="mf">27.5</span><span class="p">,</span> <span class="mf">27.7</span><span class="p">,</span> <span class="mf">34.1</span><span class="p">,</span> <span class="mf">28.1</span><span class="p">,</span> <span class="mf">21.4</span><span class="p">,</span> <span class="mf">18.6</span><span class="p">,</span> <span class="mf">12.3</span><span class="p">])</span>

<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="mf">463.</span><span class="p">,</span>  <span class="mf">360.</span><span class="p">,</span>  <span class="mf">380.</span><span class="p">,</span>  <span class="mf">584.</span><span class="p">,</span>  <span class="mf">763.</span><span class="p">,</span>  <span class="mf">886.</span><span class="p">,</span> <span class="mf">1168.</span><span class="p">,</span> <span class="mf">1325.</span><span class="p">,</span>  <span class="mf">847.</span><span class="p">,</span>
    <span class="mf">542.</span><span class="p">,</span>  <span class="mf">441.</span><span class="p">,</span>  <span class="mf">499.</span><span class="p">,</span>  <span class="mf">363.</span><span class="p">,</span>  <span class="mf">327.</span><span class="p">,</span>  <span class="mf">414.</span><span class="p">,</span>  <span class="mf">545.</span><span class="p">,</span>  <span class="mf">726.</span><span class="p">,</span>  <span class="mf">847.</span><span class="p">,</span>
   <span class="mf">1122.</span><span class="p">,</span> <span class="mf">1355.</span><span class="p">,</span>  <span class="mf">916.</span><span class="p">,</span>  <span class="mf">571.</span><span class="p">,</span>  <span class="mf">377.</span><span class="p">,</span>  <span class="mf">465.</span><span class="p">,</span>  <span class="mf">377.</span><span class="p">,</span>  <span class="mf">362.</span><span class="p">,</span>  <span class="mf">518.</span><span class="p">,</span>
    <span class="mf">683.</span><span class="p">,</span>  <span class="mf">838.</span><span class="p">,</span> <span class="mf">1012.</span><span class="p">,</span> <span class="mf">1267.</span><span class="p">,</span> <span class="mf">1464.</span><span class="p">,</span> <span class="mf">1000.</span><span class="p">,</span>  <span class="mf">629.</span><span class="p">,</span>  <span class="mf">448.</span><span class="p">,</span>  <span class="mf">466.</span><span class="p">,</span>
    <span class="mf">404.</span><span class="p">,</span>  <span class="mf">343.</span><span class="p">,</span>  <span class="mf">493.</span><span class="p">,</span>  <span class="mf">575.</span><span class="p">,</span>  <span class="mf">921.</span><span class="p">,</span> <span class="mf">1019.</span><span class="p">,</span> <span class="mf">1149.</span><span class="p">,</span> <span class="mf">1303.</span><span class="p">,</span>  <span class="mf">805.</span><span class="p">,</span>
    <span class="mf">739.</span><span class="p">,</span>  <span class="mf">587.</span><span class="p">,</span>  <span class="mf">561.</span><span class="p">,</span>  <span class="mf">486.</span><span class="p">,</span>  <span class="mf">470.</span><span class="p">,</span>  <span class="mf">564.</span><span class="p">,</span>  <span class="mf">609.</span><span class="p">,</span>  <span class="mf">899.</span><span class="p">,</span>  <span class="mf">946.</span><span class="p">,</span>
   <span class="mf">1295.</span><span class="p">,</span> <span class="mf">1325.</span><span class="p">,</span>  <span class="mf">760.</span><span class="p">,</span>  <span class="mf">667.</span><span class="p">,</span>  <span class="mf">564.</span><span class="p">,</span>  <span class="mf">633.</span><span class="p">,</span>  <span class="mf">478.</span><span class="p">,</span>  <span class="mf">450.</span><span class="p">,</span>  <span class="mf">567.</span><span class="p">,</span>
    <span class="mf">611.</span><span class="p">,</span>  <span class="mf">947.</span><span class="p">,</span>  <span class="mf">962.</span><span class="p">,</span> <span class="mf">1309.</span><span class="p">,</span> <span class="mf">1307.</span><span class="p">,</span>  <span class="mf">930.</span><span class="p">,</span>  <span class="mf">668.</span><span class="p">,</span>  <span class="mf">496.</span><span class="p">,</span>  <span class="mf">650.</span><span class="p">,</span>
    <span class="mf">506.</span><span class="p">,</span>  <span class="mf">423.</span><span class="p">,</span>  <span class="mf">531.</span><span class="p">,</span>  <span class="mf">672.</span><span class="p">,</span>  <span class="mf">871.</span><span class="p">,</span>  <span class="mf">986.</span><span class="p">,</span> <span class="mf">1368.</span><span class="p">,</span> <span class="mf">1319.</span><span class="p">,</span>  <span class="mf">924.</span><span class="p">,</span>
    <span class="mf">716.</span><span class="p">,</span>  <span class="mf">651.</span><span class="p">,</span>  <span class="mf">708.</span><span class="p">,</span>  <span class="mf">609.</span><span class="p">,</span>  <span class="mf">535.</span><span class="p">,</span>  <span class="mf">717.</span><span class="p">,</span>  <span class="mf">890.</span><span class="p">,</span> <span class="mf">1054.</span><span class="p">,</span> <span class="mf">1077.</span><span class="p">,</span>
   <span class="mf">1425.</span><span class="p">,</span> <span class="mf">1378.</span><span class="p">,</span>  <span class="mf">900.</span><span class="p">,</span>  <span class="mf">725.</span><span class="p">,</span>  <span class="mf">554.</span><span class="p">,</span>  <span class="mf">542.</span><span class="p">,</span>  <span class="mf">561.</span><span class="p">,</span>  <span class="mf">459.</span><span class="p">,</span>  <span class="mf">604.</span><span class="p">,</span>
    <span class="mf">745.</span><span class="p">,</span> <span class="mf">1105.</span><span class="p">,</span>  <span class="mf">973.</span><span class="p">,</span> <span class="mf">1263.</span><span class="p">,</span> <span class="mf">1533.</span><span class="p">,</span> <span class="mf">1044.</span><span class="p">,</span>  <span class="mf">821.</span><span class="p">,</span>  <span class="mf">621.</span><span class="p">,</span>  <span class="mf">601.</span><span class="p">,</span>
    <span class="mf">549.</span><span class="p">,</span>  <span class="mf">572.</span><span class="p">,</span>  <span class="mf">711.</span><span class="p">,</span>  <span class="mf">819.</span><span class="p">,</span> <span class="mf">1141.</span><span class="p">,</span> <span class="mf">1350.</span><span class="p">,</span> <span class="mf">1285.</span><span class="p">,</span> <span class="mf">1643.</span><span class="p">,</span> <span class="mf">1133.</span><span class="p">,</span>
    <span class="mf">784.</span><span class="p">,</span>  <span class="mf">682.</span><span class="p">,</span>  <span class="mf">587.</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="numpy-polyfit">
<h3><span class="section-number">2.9.1. </span>numpy.polyfitを用いる例<a class="headerlink" href="#numpy-polyfit" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html">numpy.polyfit</a>関数を用いると、線形回帰モデルのパラメータを求めることができる。多項式近似の次元数を３番目の引数に指定する。以下は2次関数にフィッティングする例である。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">W</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([  1.73160796, -33.14302425, 635.74825774])
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(0\)</span>から<span class="math notranslate nohighlight">\(35\)</span>までに等間隔に配置した<span class="math notranslate nohighlight">\(100\)</span>個の数値<code class="docutils literal notranslate"><span class="pre">x</span></code>に対して、目的変数の推定値<code class="docutils literal notranslate"><span class="pre">y_hat</span></code>を求めるため、<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.polyval.html">numpy.polyval</a>関数を呼び出し、回帰曲線を描画する。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyval</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">plot_graph</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/02mra_43_0.png" src="../_images/02mra_43_0.png" />
</div>
</div>
<p>続いて、3次関数にフィッティングしてみる。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">W</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 5.01164586e-02, -1.44685888e+00,  2.92887138e+01,  2.64628814e+02])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyval</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">plot_graph</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/02mra_46_0.png" src="../_images/02mra_46_0.png" />
</div>
</div>
<p>さらに、4次関数にフィッティングした結果を示す。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">W</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-4.27358629e-03,  4.12591380e-01, -1.23937689e+01,  1.67566320e+02,
       -3.47044444e+02])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyval</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">plot_graph</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/02mra_49_0.png" src="../_images/02mra_49_0.png" />
</div>
</div>
</div>
<div class="section" id="sklearn-linear-model-linearregression">
<h3><span class="section-number">2.9.2. </span>sklearn.linear_model.LinearRegressionを用いる例<a class="headerlink" href="#sklearn-linear-model-linearregression" title="このヘッドラインへのパーマリンク">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">sklearn.linear_model.LinearRegression</a>クラスを用いても、線形回帰を容易に実行できる。訓練データにフィッティングさせるには、<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.fit">fit</a>関数を呼び出せばよいが、説明変数のデータとして、事例数を行数、目的変数の数を列数とした行列形式で与えることになっているので、<code class="docutils literal notranslate"><span class="pre">X</span></code>の形状を変更する。以下に、3次関数にフィッティングする例を示す。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">reg</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">LinearRegression</span><span class="p">())</span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Pipeline(steps=[(&#39;polynomialfeatures&#39;, PolynomialFeatures(degree=3)),
                (&#39;linearregression&#39;, LinearRegression())])
</pre></div>
</div>
</div>
</div>
<p>回帰直線の係数（傾き）を表示する。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">reg</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 0.        , 29.28871381, -1.44685888,  0.05011646])
</pre></div>
</div>
</div>
</div>
<p>回帰直線の切片を表示する。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">reg</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">intercept_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>264.62881410163936
</pre></div>
</div>
</div>
</div>
<p>決定係数を求めるため、<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score">score</a>関数を呼び出す。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8784609747360882
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(0\)</span>から<span class="math notranslate nohighlight">\(35\)</span>までに等間隔に配置した<span class="math notranslate nohighlight">\(100\)</span>個の数値<code class="docutils literal notranslate"><span class="pre">x</span></code>に対して、目的変数の推定値<code class="docutils literal notranslate"><span class="pre">y_hat</span></code>を求めるため、<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict">predict</a>関数を呼び出し、回帰曲線を描画する。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">plot_graph</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/02mra_61_0.png" src="../_images/02mra_61_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="id17">
<h2><span class="section-number">2.10. </span>確認問題<a class="headerlink" href="#id17" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>データ<span class="math notranslate nohighlight">\(\mathcal{D}_s\)</span>に対して多項式のフィッティングを行いたい。以下の処理を行うプログラムを作成せよ。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">7</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
<p>なお、NumPy, SciPy, scikit-learn, statsmodel等のライブラリには回帰分析を行う便利な関数として以下のようなものがあるが、ここでは使わずに講義中で説明した式をプログラムとして表現すること。</p>
<ul class="simple">
<li><p><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html">np.polyfit</a>, <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.polynomial.polynomial.Polynomial.fit.html#numpy.polynomial.polynomial.Polynomial.fit">np.polynomial.polynomial.Polynomial.fit</a></p></li>
<li><p><a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html">scipy.linalg.lstsq</a>, <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html">scipy.optimize.curve_fit</a>, <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html">scipy.stats.linregress</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">sklearn.linear_model.LinearRegression</a></p></li>
<li><p><a class="reference external" href="https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.html#statsmodels.regression.linear_model.OLS">statsmodels.regression.linear_model.OLS</a></p></li>
</ul>
<p><strong>(1) 行列による1次関数のパラメータ推定</strong></p>
<p><span class="math notranslate nohighlight">\(y = w_0 + w_1 x\)</span>とおき、学習データ<span class="math notranslate nohighlight">\(\mathcal{D}_s\)</span>上の平均二乗残差を最小にする<span class="math notranslate nohighlight">\(\pmb{w} = \begin{pmatrix}
w_0 \\
w_1
\end{pmatrix}\)</span>を行列演算により求めよ。</p>
<p><strong>(2) ２次関数による重回帰</strong></p>
<p><span class="math notranslate nohighlight">\(y = w_0 + w_1 x + w_2 x^2\)</span>とおき、重回帰により平均二乗残差を最小にする<span class="math notranslate nohighlight">\(\pmb{w} = \begin{pmatrix}
w_0 \\
w_1 \\
w_2
\end{pmatrix}\)</span>を求めよ。</p>
<p><strong>(3) 回帰曲線の描画</strong></p>
<p>回帰で求めた2次関数をデータ点とともにグラフに描け。</p>
<p><strong>(4) 決定係数</strong></p>
<p>回帰で得られた2次関数の決定係数（<span class="math notranslate nohighlight">\(R^2\)</span>）を求めよ。</p>
<p><strong>(5) 3次関数による重回帰</strong></p>
<p><span class="math notranslate nohighlight">\(y = w_0 + w_1 x + w_2 x^2 + w_3 x^3\)</span>とおき、重回帰により平均二乗残差を最小にする<span class="math notranslate nohighlight">\(\pmb{w} = \begin{pmatrix}
w_0 \\
w_1 \\
w_2 \\
w_3
\end{pmatrix}\)</span>を求めよ。</p>
<p><strong>(6) 回帰曲線の描画</strong></p>
<p>回帰で求めた3次関数をデータ点とともにグラフに描け</p>
<p><strong>(7) 決定係数</strong></p>
<p>回帰で求めた3次関数の決定係数（<span class="math notranslate nohighlight">\(R^2\)</span>）を求めよ</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "chokkan/mlnote",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="01sra.html" title="前へ ページ">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">前へ</p>
            <p class="prev-next-title"><span class="section-number">1. </span>単回帰</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="03regularization.html" title="次へ ページ">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">次へ</p>
        <p class="prev-next-title"><span class="section-number">3. </span>モデル選択と正則化</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          <div class="extra_footer">
            © Copyright 2020-2021 by 岡崎 直観 (Naoaki Okazaki). この作品は<a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/">クリエイティブ・コモンズ 表示 - 非営利 - 改変禁止 4.0 国際 ライセンス</a>の下に提供されています。 <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="クリエイティブ・コモンズ・ライセンス" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png" /></a>　ただし、作品中のコードセル部分は<a rel="license" href="https://opensource.org/licenses/MIT">MITライセンス</a>の下に提供されています。

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>